{
  "year": 2025,
  "projects": [
    {
      "id": 1,
      "slug": "memoirs-of-a-22-yo",
      "title": "Memoirs of a 22 yo",
      "description": "Use AI to write a memoir of my short life, with a goal of ~50 pages or 12,500 words. Chatbots like ChatGPT are much faster and better writers than I am, so I will come up with memories and themes I think are important to describe my life journey, and I will use AI to do most of the writing and rewriting for me.",
      "artwork": "project_artwork/projects_2025_cover_img_1.webp",
      "color": "#0000ff",
      "dateStarted": "2024-12-30",
      "dateFinished": "2025-01-16",
      "triumphs": "In its final form, I have a memoir of 21.5k words and (when formatted like a novel) 80 pages, well above the quantities of the goal I set out with.\r\n\r\nAfter some trial and error with prompting, ChatGPT did a good job of turning my drafted pieces of each chapter into a well-written section of prose -> this streamlined the writing process greatly.\r\n\r\nOver time, I got better at writing my drafts with the necessary details and bits of reflection and emotion, such that chat was more equip to expand each draft into a complete narrative.",
      "pitfalls": "Due to context length limitations in chats, ChatGPT wasn't much help in organizing the flow of the book, and I often didn't do a great job of this myself as I was drafting each chapter, so the book bobs around a bit and only loosely is pointed at any particular themes. \r\n\r\nBoth using the Canvas tool (which I gave up on after the first few sessions) and having random factoids from my drafts constantly being added to memories were inconveniences of using ChatGPT.\r\n\r\nGiving chat my drafts bit by bit lead to it producing a lot of repetition, poor transitions between paragraphs, and a lack of connecting stories to the common themes in the book; and I didn't give myself quite enough time to do as good of a review and rewriting process as I would have liked.",
      "logs": [
        {
          "date": "2024-12-30",
          "pre": "I will discuss with o1 how I can go about collaborating with today's chatbots to produce a memoir of my life, then I'll spend some time creating a memory dump of different events and themes that I think are important to illustrate my story- I can then experiment with having chat write some sections or otherwise focus more on building out the structure of the book (chapters, themes, etc).",
          "post": "It looks like the canvas \"collaborative writing\" tool with ChatGPT will be perfect for writing a short book together; I have some general chapters that we've outline (childhood, mamba mentality, dropout, learn to code), and there's about 1k words about my struggles in kindergarten-1st grade -> next session I will want to expand on some of the other trends that happened through elementary and middle school."
        },
        {
          "date": "2024-12-31",
          "pre": "I'll start by reviewing the few paragraphs we wrote about my very early days in school, then I'll do some brainstorming on my own of different moments and some helpful reflective commentary about the rest of my elementary and middle school years -> once I have a nice collection of thoughts I'll pass them over to chatgpt and go through the drafting-revising cycle.",
          "post": "I fleshed out a good deal of the Childhood chapter, adding in stories throughout the years around my interests in school, the different times I had gotten in trouble, some stories about the friends I was hanging around with, and ending with memories about my 8th grade basketball season; sending chat a full list of stories all together doesn't work great because it won't expand the stories in a single shot beyond a certain point, so I think I should really spend time building my own structure to chapter 2, creating a solid list of memories from the time and some of the main themes that I would like included, then feed these piecemeal to chat (at about 8 pages in novel format, ~2000 words)."
        },
        {
          "date": "2025-01-01",
          "pre": "I will focus on creating my own outline for what chapter 2 \"Mamba Mentality\" should include-> make a list of memories, organize then thoughtfully (maybe add some comments about why it is ordered the way it is), make notes throughout about what was going through my mind at the time and also my perspective looking back, and write about the themes I think the chapter should emphasize -> then give the outline to chatgpt and start feeding it the stories one by one to expand and write.",
          "post": "This went much smoother today -> I wrote up some of the major events that took place during the ages of 14-16, trying to include the essential details that would be need for chat to expand them, and also tried to include more emotional and reflective thoughts; I finished my draft of chapter 2, pasted it into a separate chat for o1 to create an outline of the chapter and capture the main themes to emphasize, then gave this outline as context and started feeding it bits and pieces of my draft; about half through chapter 2, 11 pages and 3k words."
        },
        {
          "date": "2025-01-02",
          "pre": "I'll continue feeding chat my stories for chapter 2, I also might add some more details for the second half of the chapter to better cover my social life at the time and also to foreshadow chapter 3 by adding a section of reflection on how this period of my life affected my course -> then I'll move on to drafting chapter 3 \"Dropout\".",
          "post": "I finished chapter 2, adding in some small details but leaving the social aspects for chapter 3; I've started drafting chapter 3 now, talking about how I got introduced into vaping and telling my first 2 weed stories, also my 2 suspensions, but also how in 10th grade I started making new friends again; there are many more drug related stories I can add, and other subjects like working at BK, 'dating' Belle Stiller, covid and starting to write, and then dropping out- should be a good chapter ;) @ 17 pages and 4.6k words."
        },
        {
          "date": "2025-01-06",
          "pre": "I'll start by rereading the stories I've so far drafted for chapter 3, then just continue writing -> as your brain dump, consider the themes that seem most important for the chapter, then also what stories best highlight these and how to organize the flow of the chapter, and don't forget to include reflections throughout or maybe at the end of what affect this period of my life has on me today.",
          "post": "I continued writing my story drafts for chapter 3, which currently sits at 4.5k words all on its own; I told some more stories about the different drug shenanigans I was getting into in 10th grade, a little bit about my brief stint with dating Belle, and then a long story recounting stealing the percs on spring break; next session I'll try to wrap up drafting chapter 3 and begin having chat rewrite it."
        },
        {
          "date": "2025-01-07",
          "pre": "The goal for today is to finish drafting chapter 3, and hopefully to start working with chat to write its draft of the chapter -> as today marks the halfway point in the project, I think my 2nd half will entail trying to cruise through and get my drafts and chat's first draft of the book done as soon as possible, this way I can have some time to reread and think about how I may want to restructure, add or remove pieces, and also to beef up any narration.",
          "post": "I finished my draft for chapter 3, it makes for ~7.5k words on its own; I wrote about my friendships with Jose and Logan, my stints working at BK, smoking more weed, getting into writing during covid, and how I eventually came to drop out of college; I used chat to make an outline of the chapter, and next session I can start by having chat transform my drafts."
        },
        {
          "date": "2025-01-08",
          "pre": "I'll start by working with chat to write the first draft of chapter 3, the largest and most interesting chapter of the memoirs so far; once this is done, I'll just move on to drafting chapter 4 \"Learn to Code\" -> can talk about my experience living with Logan, briefly with Carson, then moving back home and finally getting my own apartment, the pitfalls and triumphs of my relationship with Emerald, investing and learning math, crypto and ai.",
          "post": "It was a frustrating start to rewriting with chat, but once I stopped using the Canvas tool and started including brief instructions and context before giving it each section, things started flowing; I'm finished with a first draft through chapter 3, am sitting at 50 pages and ~13.5k words (my original goal for this project), and will start drafting chapter 4 \"Learn to Code\" next session."
        },
        {
          "date": "2025-01-09",
          "pre": "Today is all about drafting chapter 4 \"Learn to Code\"; I think I will start by creating a list of general topics that were most important, then breaking these down into sublists of specific stories that are relevant to the topic, next think about the themes that the chapter should emphasized based on this brainstorming, and from here you can organize the flow of the chapter better than previous chapters where I just winged it -> then, start drafting the stories.",
          "post": "I have about 2000 words drafted so far for chapter 4, talking about my experience living with Logan and how our relationship degenerated over time, my job at Legacy (pretty brief/non-descript here), and some of my initial forays into investing; next session I can continue with writing-> living with carson and then my parents, getting my own apartment, Milady Twitter, the whole shebang with Emerald, and starting to grind on programming/building cool shit."
        },
        {
          "date": "2025-01-13",
          "pre": "This is the last week of writing my memoirs; today I will try to finish my draft of chapter 4, focusing on being more selective with the stories I choose and then doing a better job of explaining and \"showing not telling\" them-> next session then I'd be drafting with chat, and the final 2 work days would be for review and editing.",
          "post": "I wrote another 2.5k words for the draft of chapter 4, expanding on section for working at Legacy, and writing sections for living with Carson, my parents, and then eventually getting my own apartment, and I am midway through talking about Emerald -> I can also explain more about digging into programming, and maybe end with a short section about how I see things today and what  I think the future will bring."
        },
        {
          "date": "2025-01-14",
          "pre": "I need to finish drafting chapter 4, really just covering the last 18 months or so, and then I can decide if I want to end the memoirs with a short forward-looking section, but then begin drafting with chat.",
          "post": "I completed the draft for chapter 4 \"Learn to Code\", and am down to the last 2 paragraphs to send to chat for rewriting; the memoir stands at 22.5k words and 79 pages currently, so the next two sessions I will be rereading and reworking the book -> I think I'll just read the whole thing myself, making comments along the way, and then workshop the changes with ai."
        },
        {
          "date": "2025-01-15",
          "pre": "I need to send my last 2 paragraph drafts to chat for the rewrites, and then the first full draft of the book will be complete; then I think the thing to do is to read through the entire manuscript, making comments in the docs along the way with specific edits to be made for changing the tone or to correct wrong statements, or also if a section should be removed completely or if a new story should be added to support the theme.",
          "post": "Well I started rereading but only made it to page 28 or 80-some; there are mostly minor edits I want to make, so I think if I try to really breeze through reading and making these comments next session, I should be able to touch things up pretty quickly with some ai assistance."
        },
        {
          "date": "2025-01-16",
          "pre": "This is my final day working on my memoir project, and there is a long ways to go if I want to try to edit the whole thing-> so I'm going to speed read the rest of the book, leaving informative but simple comments (unless I want to include a new story somewhere), and try to leave myself some time to comb over my comments with ai to make the edits.",
          "post": "BOOM!!! I finished reading through my short memoirs, revising the most egregious parts and leaving the rest (I ended up doing the rewrites myself, Gemini didn't seem like it'd be too helpful)-> in total, there are 80 pages and 21.5k words- not too shabby ^^"
        }
      ],
      "images": [
        {
          "path": "project_images/Screenshot_2025-01-20_201259.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-01-20_201338.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-01-20_201638.png",
          "caption": null
        }
      ],
      "tasks": []
    },
    {
      "id": 2,
      "slug": "scikit-learn",
      "title": "Scikit-Learn",
      "description": "To explore regression and classification problems using basic learning algos: linear regression, logistic regression, decision trees, support vector machines (SVM), k-nearest neighbors (KNN); and datasets: Titanic Survival and Boston Housing Prices. I will need to preprocess the data, experiment with feature engineering, and use performance metrics to compare the predictive capabilities of each model on each dataset.",
      "artwork": "project_artwork/projects_2025_cover_img_2.webp",
      "color": "#ff0000",
      "dateStarted": "2025-01-20",
      "dateFinished": "2025-02-08",
      "triumphs": "Explored the basics of many different algos used for ML: linear and logistic regression, kNN, SVMs w/ different kernels, decision trees, random forests, and different gradient boost algos, and finished with MLP.\r\nI worked across 3 different datasets: Boston House Prices, Titanic Survival, and Adult Income; this required me to fill missing values or drop features in some cases, convert categorical features into numeric ones, create feature interactions, do standardization, etc. \r\nI asked chat to explain a lot of the basics of each algo, what tuning strategies are available, how the cost functions and 'learning' work.",
      "pitfalls": "Missed an opportunity to collect your own data on a problem that interests you, and then figure out what type of algorithm gives you the most insight into this data.\r\nSometimes I feel bad just copy pasting code from chat, even when I take time to look it over and ask for clarification on parts I don't understand -> I could have spent time getting in reps at writing the code line by line for myself, and having chat correct and explain my errors. \r\nI could have wrestled with the math a little more, asking more questions to chat, trying to put things into my own words, and even trying to work through simplified examples by hand to try to grok how they work.",
      "logs": [
        {
          "date": "2025-01-20",
          "pre": "I spent most the of the session planning this project with chat, for now I will get sci kit learn and the boston housing prices dataset into colab, try to explore and become familiar with the data, and maybe play around with a first go at linear regression.",
          "post": "I downloaded the Boston dataset (scikit-learn removed it for racism, so I used Keras), made it into a pandas df, renamed the cols to their real values, and then used .describe to view the max/min, mean and std of each col; also chat helped me to train it 1 time without any feature selection or preprocessing (it took 0s!)."
        },
        {
          "date": "2025-01-22",
          "pre": "I should start with further exploration of the dataset (discuss with chat what I should be looking for, and how you will leverage the data to build learning models), then get to work using basic linear regression of boston house prices; record your experiments and results.",
          "post": "I explored the dataset in a few ways: running regression on each feature by itself, using z-scores and boxplots to view outliers in each feature; and also I tested out both standardization and normalization (no effect on regular linear regression) -> by removing some outliers, I've gotten to 12.5 MSE and 0.75 r^2."
        },
        {
          "date": "2025-01-23",
          "pre": "I'll finish up on my linear regression for boston house prices by adapting some of the features to logarithmic curves (as chat keeps telling me to try), then I should also experiment and learn about lasso and ridge regression; after this, I can try to break the house prices down into buckets and begin testing out logistic regression.",
          "post": "I tried using log(x+1) and x_sqrt for transformations (log1p was a little better), and got my best results yet by dropping some outliers and selectively applying log1p -> mse 9.1, r2 0.81; I also looked into L1 and L2 regularization, and used grid search to find the best alpha values, but these didn't have much effect on the performance; finally I started exploring multicollinearity, which I'm not understanding yet, but creating new cols via interactions may be able to improve the model some."
        },
        {
          "date": "2025-01-26",
          "pre": "I will explore finding relevant features to combine into new interactive dimensions, and then look for more ways to evaluate the data around missed predictions, then I'll start in on breaking the house prices into \r\nbuckets and applying logistic regression methods.",
          "post": "I found that adding squared features for TAX and LSTAT improved linear regression baseline performance significantly (I couldn't reproduce the preprocessing numbers I had last time for some reason), I then broke down the prices into 3 bins and was getting about 78% accuracy overall -> I tried regularization and data balancing (not all categories have equal examples), but in the end the choice of scaler was the only thing to have a real effect (using 5 bins made it even worse, as expected); next session I'll begin on knn."
        },
        {
          "date": "2025-01-27",
          "pre": "Before moving on to kNN, I want to try logistic regression again but for binary classification, and I want to see with my own eyes that this algo will give off probabilities and not labels for classification (also I've done some data exploration around every feature EXCEPT MEDV, which is ridiculous); then I should go deep with chat into building context around kNN algos and start applying that to the data.",
          "post": "I tried out logistic regression for both binary and unary classification, I explored MEDV values and distributions with histograms, box plots, and panda's describe function (learning about some basic stats along the way), I tried making some interaction features which did help significantly on binary classification; finally, I setup kNN regression and using standardscaler and k=3 I've already blown linear regression out of the water with MSE=5 and R^2=.9."
        },
        {
          "date": "2025-01-28",
          "pre": "I'll front load the explanations and context today by having chat explain kNNs in detail, then experiment with different methods and hyperparams to try improving my accuracy both on regression; if there's time, I'll begin learning about decision trees with chat and try to get some baselines.",
          "post": "I achieved 2.34 RMSE with my tuned kNN, meaning on average my predictions were off by $2,340 (not bad!); I learned about and explored different ways to measure distance and mean, and also using pca which helped considerably, and I started learning a bit about how decision trees work."
        },
        {
          "date": "2025-01-29",
          "pre": "I'm going to tackle decision trees today, I'll start by reviewing the description chat started me on last session, and then start experimenting with different hyperparams and data visualization; then can also implement a random forest ensemble model to see if this improves performance.",
          "post": "Decision trees and random forests were pretty bad performers on this dataset (mse: 29.4 and mse: 13.6 respectively, both notably worse than kNN and linear regression), and interaction features helped only a little bit on random forests (otherwise these models were fairly unphased by my changes away from the default); SVM works better for classification, so I can leave that for the Titanic dataset- but I can experiment a little more around other ensemble model combinations."
        },
        {
          "date": "2025-02-01",
          "pre": "I'm going to jump straight into the Titanic dataset today; starting with data visualization, I should look at the classes and their distributions and other stats, look for outliers, size of data, and look especially closely at the target class; then, start at ground zero, applying and tweaking logistic regression.",
          "post": "I looked at the data (a mix between categorical and numerical features), decided to drop the 'deck' col bc it missed so many values, and then used mean and mode to fill in values for age and port; I tried to do linear regression against single features, then a simple logistic regression which got 82.7% acc after adding in age^2 and fare^2, and moving the threshold to 0.67."
        },
        {
          "date": "2025-02-03",
          "pre": "I'm going to start today by taking a good look at the code I've been using throughout this project and asking chat to explain areas where I don't know what we're doing; then I'll work on applying and tweaking decision trees, random forests, and gradient boosting algos on the data (82.7% is the number to beat!)",
          "post": "My best classification tree got to 82.1%, and my baseline for random forest already topped all other tests with 83.2% acc (I just did a random grid search though and will need to test this out next session), and then I will approach gradient boosting (this was also the most confusing in the 100 pg ml book, so I'll have chat try to explain it to me) -> also found my next book: \"Grokking Algorithms\"!"
        },
        {
          "date": "2025-02-04",
          "pre": "I'll start by reviewing my experiments with random forests and see if I feel I've maxed out that algo, then I'll discuss gradient boosting with chat and implement this on Titanic; I have yet to build a SVM model, so I would like to have chat school me on this and try it out as well.",
          "post": "My random forest model didn't get any better, but I tried a few gradient boosting algos (sklearns basic GB, XGBoost, and LightGBM) and got to my best acc yet using simple GB with 500 trees, 5 max depth, and 0.0075 lr; I'm really learning to appreciate being able to use gridsearch to tune these models; next session I'll start with a larger dataset called Adult Income, and focus on tuning some tree-based models as a baseline, and then build a simple neural net for comparison."
        },
        {
          "date": "2025-02-05",
          "pre": "I want to take a moment to build a svm for Titanic and walk through what it's doing with chat and also discuss tuning strategies; then I'll move to the Adult Income dataset and start by exploring the data, and then try out some try algos on it.",
          "post": "I tried a few types of SVM models on the Titanic Survival dataset (RBF, Linear, Polynomial, and Sigmoid) -> polynomial did the best giving 82.1% with some tuning; I've downloaded the Adult Income dataset for building neural networks on next session, and we've done some basic data visualization and preprocessing (hot-one encoding for categorical features, normalization for numerical features)."
        },
        {
          "date": "2025-02-08",
          "pre": "The last day of this sckikit learn project, I want to have chat slowly walk me through the implementation of neural nets on the adult income dataset; we will build some basic models, talk about hyperparams and tuning strategies, ways to visualize what the model is doing, and to explain the basic mathy stuff behind gradient descent, backprop, etc.",
          "post": "I worked a little on a simple MLP model, getting up to 85% accuracy by mostly just trying out different layer X neuron counts; I also had chat explain MLPs in detail, and added some visualizations (confusion matrix as per usual, and also plotting the loss curve [I was doing validation set/loss_curve too, but it was too slow])."
        }
      ],
      "images": [
        {
          "path": "project_images/Screenshot_2025-02-08_102032.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-02-08_102559.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-02-08_102726.png",
          "caption": null
        }
      ],
      "tasks": []
    },
    {
      "id": 3,
      "slug": "news-aggregator",
      "title": "News Aggregator",
      "description": "To build a live, personal website that can source, organize, alert, and summarize a curated stream of news for me everyday. It will involve using Vercel to freely host the website, finding relevant sources of news and scraping titles and links to display to my site, giving alerts when new blogs have been posted from various sites, using OpenAI API to summarize articles and to determine stories I'd find most interesting, and to have this happen automatically every day (without costing me a ton of money!).",
      "artwork": "project_artwork/projects_2025_cover_img_3.webp",
      "color": "#ff0000",
      "dateStarted": "2025-02-10",
      "dateFinished": "2025-03-01",
      "triumphs": "The site is hosted for free with Vercel, and I can access it on my phone or anywhere else; the database and backend scripts (for scraping and OpenAI API) are also freely hosted with Supabase and Render respectively.\r\nIt is a fair replacement of my old news-aggregation system (automating certain links to be opened via Apple shortcuts), but also allows for the links to be updated nearly seamlessly from the site, and there's a page to host a library of other relevant links, plus a working feature to randomly display 2 of these along with the daily links.\r\nThis was my first experience with using OpenAI's API, or any ai API for that matter, and this serves as an inspiration and stepping stone for future projects.",
      "pitfalls": "The scraping is limited (only data from Star Tribune's RSS feed is coming in from the my API), and it isn't automated due to Render's free-tier limitations and must be manually activated via clicking buttons on the site to receive new data.\r\nIntegrating responses from the OpenAI API was a bit of a pain, and as of writing this the feature is currently broken -> also, the initial intent was to either do summaries or recommendation/ranking of articles- I went with ranking, but adding multiple sites resulted in chat only picking articles all from the same site; also, updating the prompt could bring unintended results; all in all, I didn't give myself enough time to really make a useful ai integration into the site.\r\nThe admin portion could use a lot of cleaning up: a login system, collapsible sections, fixing the buttons for reordering links.",
      "logs": [
        {
          "date": "2025-02-10",
          "pre": "The goal is to spend some time talking with chat and coming up with a roadmap and realistic MVP to shoot for, and to also uncover as many of the pitfalls I might encounter from the onset so I can route around them -> then get onto Vercel and start creating a bare-bones implementation of the site, and maybe pick a few data sources to feel things out with.",
          "post": "I did some planning with chat (there is a lot going on with this project) -> I set up a new GitHub repo, started a NextJS app, and then deployed the project onto Vercel (and can actually see open the url from my phone!); I think the next step should be to actually scrape a few sites, get them posted to my news site, and make the UI a little more manageable >> the only way to eat an elephant is 1 bite at a time!"
        },
        {
          "date": "2025-02-11",
          "pre": "The first thing I'd like to do is to be able to scrape a site for whatever news stories they are posting, and get the title and link onto my site -> establish initial lists of sources to draw from and host these links somewhere on the site.",
          "post": "FUCKKKK, this project is kicking my ass -> I started working with copilot in vscode, and using Puppeteer we were able to scrape and display from one site - until we weren't; I rolled back to site to the bare bones from GitHub, then created a new flask application to use in tandem with beautiful soup for scraping, and also set up a project on Supabase for the db (yet to see if this is actually working)."
        },
        {
          "date": "2025-02-12",
          "pre": "We will keep chipping away today -> to start I want to talk with chat and reach an understanding of what different parts of the application are for (nextjs vs supabase vs flask, etc) and how data will flow through them; then, I should try to scrape something (anything!) using flask and beautifulsoup, and post the data to supabase, and finally for next js to retrieve and display the data on my site -> if this can be accomplished, we are in serious business, and then I want to create sections for the site to display daily links (like my current news aggregator via iPhone shortcuts), and also to display all links by category.",
          "post": "A very successful and delightful day hacking -> we used flask and beautifulsoup to scrape a title along with the timestamp from a toy site, then published this in a supabase db, and got nextjs to retrieve and display this in a table -> then redeployed vercel and can see the scraped data in the live site! (flask needs to get hosted like nextjs did, so the scraping can occur automatically); also, split the github repos - one for frontend and one for backend."
        },
        {
          "date": "2025-02-14",
          "pre": "First things first, we should get Flask API hosted so it can automatically scrape and post to supabase; then I want to do some wire-framing and brainstorming -> I think it would be valuable if it could replicate the ios-shortcuts news aggregator I already have (also, see if there's a possibility of adding an admin site to the project), plus a catalog that lists even more links by topic, and maybe also a place to scrape and post new blog articles; also, consider where ai might be useful within this project.",
          "post": "A bit of a shit show today, but I after trying out Railway and Fly, I eventually got Flask hosted on Render; cron jobs (1 limited per day) should be set up on vercel to run at 4AM every morning to do the latest data scrape and I'll cross my fingers when I check tm ;) if all is well, the infra should be more or less set up, minus any ai applications and maybe figuring out supabase or an alternative for a makeshift admin role, and then I can start populating the site."
        },
        {
          "date": "2025-02-17",
          "pre": "Well the automated web scraping is apparently not working yet, although things are working (render, supabase, and vercel) when I manually reload my hosted scraping page via render -> I'll start by trouble shooting this a bit and see if we can't find a nice solution since things seem so close (also coordinate/fix the timestamps across these sites), but then I want to focus on replicating the current news aggregator strategy of daily-links on my hosted site, as this will make it useful immediately.",
          "post": "Today didn't go very smoothly -> for using Render to automate the scraping, the render site has a 50s wake up period, and vercel only offers 10s of execution time for cron jobs (either can switch to using github actions or add a ping site that will keep render active and not let it sleep); I tried to start adding the daily news links in, but we are having problems getting the nextjs api to read from the supabase table -> this can go either way, either we can edit links directly in the tables stored on supabase, or add an admin site to nextjs that allows for editing (w/ changes sent to supabase)."
        },
        {
          "date": "2025-02-18",
          "pre": "We will simply try to grind out some progress today: the primary goal is to build a halfway admin page (I won't worry about credentialling or logins rn) that allows me to edit the daily links from the live page -> hopefully if we update the rls security of the table in supabase, we will then be able to write to the hosted db; get this functional, and then add content for each day and then wireframe the UI.",
          "post": "Progress was made! we now have an admin page on the hosted site that definitely allows for adding links, editing and removing current links is still a work in progress though; on the homepage, it will automatically bring you to the links of whatever day it is, and there is a dropdown menu where you can pick another day to see it's links; there is some troubleshooting needed (and thoughts on how to best make it accessible) to edit the daily links sections, then I should fill in the links, and also move on to building the catalogue of links page."
        },
        {
          "date": "2025-02-19",
          "pre": "I'll start by troubleshooting two issues 1) vercel had an error in deploying the latest commit 2) displaying the daily links in the admin page; once these are cleared up, I should be able to fill out the daily links and successfully replicate my current news aggregator -> then move on to building and populating the catalogue page.",
          "post": "Vercel deployment is up and running, with all the daily links filled in, a working dropdown menu to select what day you want to see, an admin that allows adding, removing, editing, and ordering (this last one works within bounds) of the links; you could build a catalogue page, but I think trying to tackle a scraping + ai-recommendation system is probably the highest value add thing to go after."
        },
        {
          "date": "2025-02-20",
          "pre": "The focus for today is to build out the catalogue page, and the accompanying admin abilities, and then get as far in populating and organizing its content as possible -> next week, to finish off this project, I will try my best to insert ai (start by building scraping capabilities [possibly for blogs, or for google searches], then either can be a recommendation system based on source and title, and/or based on up or down votes on my site).",
          "post": "Built a sweet catalogue page that lets you filter by category, or search by name or topic; built the catalogue admin, with add/edit/delete abilities -> uploaded 64 links to the site; generally made the site more navigable by adding buttons (should allow for collapsible headers in admins, and possible update the card situation in catalogue, also the site is no longer nicely tuned for mobile); everything is deployed to vercel- SWEET!"
        },
        {
          "date": "2025-02-24",
          "pre": "The goal for the week is to push forward on ai integration: discuss with chat for feasibility measures, and decide what direction to go (ranking [my current bias], or summarizing [which could be valuable in select places]); then I think the first step is to pick a useful but constrained topic to get scraped, and get the infra right to allow for ai to do its magic -> publish results (supabase, something else?) and display on vercel w/ nextjs.",
          "post": "Not a bad day, I spent a lot of time trying to get my .env untracked from github (failed!), then switched gears and set up an openai api account with $10, created a script to have it rank 5 phony article titles and post these to supabase (supabase is so slick), and struggled a bit but now have the ranking live on render (will rerank whenever the render rank url is refreshed) -> need to get these rankings from supabase and displayed on vercel, and then I can actually start scraping and ranking real sites!"
        },
        {
          "date": "2025-02-26",
          "pre": "I already have a simple scraper live on render and publishing to supabase/vercel, so I should set up the ranking script to get the daily scraped articles (fix the scraper first to grab from a few real-potential sites) and return its top 5 ranked -> this is what should get displayed to site; after this, I can add a button to render site to news aggregator, and implement strategy to only allow 1 scrape + rank per day.",
          "post": "Ok that was a lot of troubleshooting, I updated the scraper to draw from the star tribune business rss feed,  then combined the scraper and ranker scripts into a single file (render only allows 1 app running at a time); right now I'm firefighting the chatgpt api response -> it is responding (i can see it in render logs), but the api url isnt working and the data isn't being pushed to supabase ><."
        },
        {
          "date": "2025-02-27",
          "pre": "I feel like I'm very close to getting the chatgpt api working with the scraped articles -> rn there is some json error, so if I iron this out hopefully it'll be good to go; then, I will go out and try to find very useful sources that provide convenient RSS feeds, or I will create a scraper with them, and just try to make the ranker maximally useful within the site.",
          "post": "We managed to get the openai api to rank the articles and post to supabase, nextjs displays them now, I also added buttons to the homepage to bring you to the scraping and ranking urls, and added a feature that is supposed to delete entries in scraped and ranked supabase tables after 1 day; I started adding a feature to randomly display 2 links from the catalogue as well each day, running into same api issues but this is hopefully a quick fix."
        },
        {
          "date": "2025-03-01",
          "pre": "I just tested the supabase scraping and ranking deletion-repopulation cycle, and it's working beautifully; I want to finish the serendipity section that gives 2 random links from the catalogue; with this, I could hunt for a few more sites to try to add to the ranking system.",
          "post": "Well we've gotten it working nicely to sample 2 articles each day from the calatogue and display these; I tried adding eventbrite to the scraping function, but chat would then only return articles from there and not also star tribune, so I reverted back to the original; also added links to the ranked articles section."
        },
        {
          "date": "2025-04-21",
          "pre": "This week, my goal for the news aggregator: generally, create a deeper integration of ai into the site -- have a random country generator, and then have openai api create news summaries for a few each day; fix the scraping section to give at least some links from local/mn news (ai selected and ranked ideally); today, I'll start with the random country news generator.",
          "post": "We made a supabase table that stores 250 countries/territories, along with their populations, capital city, and their flag -> now we are trying to get the front end to grab 2 countries from the top 50 population, and 2 more from the bottom 200, but this is giving us some errors for the moment."
        },
        {
          "date": "2025-04-22",
          "pre": "The countries and their data is in supabase already -> we need a function to grab 2 countries from the top 50 and 2 more from the bottom 200, then send these over to OpenAI API and experiment with prompts to have it web search and give a nice, bulleted summary of recent news of each country, and finally put this data into supabase (temporarily, it should clear out every time there's a new submission) and have Flask pull it into the website.",
          "post": "HA, we've made some progress: now, the countries are grabbed from supabase and displayed with their data on the homepage, and then we integrated OpenAI API using gpt-4o to get the latest countries, search for 3 news headlines for each (economy, politics, and society), and this actually works and displays the data to the site -> BUT, I spent ~$1.18 on the api today, which is not sustainable; next session, we need to look into where the costs are coming from, and also have chat send its responses to supabase and have flask pull from there, and also set it up so that no matter how often you reload the site, it will only pick and report on random countries once per day."
        },
        {
          "date": "2025-04-25",
          "pre": "First thing is to look into openai api dashboard and see if i can track down what was running up the costs so much last session (if 1m tokens costs a few cents, idk what lead to $1.2 being spent on compute)- can drop from 4 countries per day to 2, and from 3 headlines to 2 (economy and politics) per country; additionally, we want chat's responses to somehow only be solicited once per day (i think put its responses into supabase, and then check for when the last responses were recorded before calling openai again).",
          "post": "Ok, we first were experimenting with different apis to get headlines from (is seems like web search tool was running up costs with openai)- tried gnews, and newsdata.io, but ended up settling with SerpAPI (google news api); also updated the random countries to only get 1 in top 50 and 1 in bottom 200 each day; now have been refactoring main page.tsx, so that most of the logic for the countries will be in its own file dailyWorkflow/route.js (currently am getting a json parsing error from chat, but it actually looks like it might be giving its responses to supabase anyways || also, need to update the home template)."
        },
        {
          "date": "2025-04-25",
          "pre": "I want to finish up on this country feature -> make sure all the data is getting stored into each database (and checking that it doesn't call mult times a day) and displaying to the homepage without errors, then push everything to github, redeploy on vercel, and check things are working from your phone; then, id like to update the future projects section of this project journal.",
          "post": "Well i got the new workflow and project refactor working-- in production; then when i went to push to github and redeploy to vercel, tons of problems started popping up, eventually trouble shooting led to basically breaking the site, so i rolled everything back to the original ><."
        }
      ],
      "images": [
        {
          "path": "project_images/Screenshot_2025-03-03_201424.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-03-03_201446.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-03-03_201346.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-03-03_201306.png",
          "caption": null
        }
      ],
      "tasks": []
    },
    {
      "id": 6,
      "slug": "french-crashcourse",
      "title": "French Crashcourse",
      "description": "Use ChatGPT as your tutor and exclusive learning resource to become as fluent and comprehending in the French language as possible in 3 weeks. Experiment with different prompt strategies and learning-retention methods, plus multi-modality -> look for methods that can be applied towards the learning other topics.",
      "artwork": "project_artwork/projects_2025_cover_img_4.png",
      "color": "#0000ff",
      "dateStarted": "2025-03-10",
      "dateFinished": "2025-03-29",
      "triumphs": "Unquestionably, I am much more knowledgeable about the French language, vocab, and pronunciation than I was coming into the project. \r\nI adapted my learning process over time, starting with using chat exclusively for vocab, and textual and audio conversations; then I started mixing in Duolingo and also listening to real French speakers (the former was very helpful, the later very difficult but also can't be neglected).\r\nThere were times where working with Chat 4.5 really was fun, or where it gave me a laugh; asking it to roast my attempts was a great learning strategy!",
      "pitfalls": "I took French fluency test before and after: 9/40 for the first go, and then somehow did worse with 8/40 at the finale.\r\nChat will talk to you in whatever language you'd like, but it cannot 'hear' your pronunciation - if you are saying things wrong, but are still interpretable, it will not correct you.\r\nFor the most part, I was leading the lessons and deciding on the curriculum path; it would be nice for things like this to be able to give chat the context of previous convos, along with your goals, and it will be a wise tutor and handle the pedagogy, so that you can just focus on the learning.",
      "logs": [
        {
          "date": "2025-03-10",
          "pre": "I'll start by running a war room with chat to come up with a course outline for how to approach learning a new language, and also different techniques I might use for learning and remembering the information -> then, I'm just going to dive in!",
          "post": "This is going to be a fun challenge! We started by going through the alphabet, then a couple tables worth of common words and phrases, and finished things off with numbers -> I think I'll try a little harder next session to get chat to take the lead; using voice is convenient, but it can't actually hear my pronunciation to correct me (should look for a ai-voice-translate app that will do this); also, I took the ESL French fluency test before starting, and got 9/40."
        },
        {
          "date": "2025-03-11",
          "pre": "Today's plan is to review the alphabet, numbers, and couple words and phrases I learned yesterday -> then, we will begin to explore common verbs and pronoun usage + some more general vocab.",
          "post": "I had chat teach me about the common pronouns, and also the verbs etre (to be) and avoir (to have); then I set up my Doulingo for French (copping out I know, I think to try driving forward with chat to start, and then plow through more vocab on Doulingo is the way) and completed the first lesson."
        },
        {
          "date": "2025-03-12",
          "pre": "I'll start with chat: reviewing the alphabet and numbers again, then do some general vocab (colors, days of the week, months, simple nouns (people, food, places)), review verbs etre and avoir, and then ask chat for a few more; then, crank out Duolingo!",
          "post": "I have the alphabet and numbers 1-20-ish down pretty well, I glossed over some nouns and also got introduced to faire (to do); on Duolingo I was making good progress but ran out of hearts, so the last 15 minutes was chat teaching me French slang."
        },
        {
          "date": "2025-03-14",
          "pre": "Mixing things up a little bit, I'm going to start by reviewing with chat/on my own (alphabet, numbers, basic greetings/phrases); then I'll grind out Duolingo; then come back to chat, maybe can ask for some extra basic vocab, but focus on learning and reinforcing verb usage, and practicing reading/writing/speaking/listening with all the vocab you've been learning.",
          "post": "This new flow worked out good, I did some review online and from my notes, then tried writing out soem basic phrases to chat; grinded out Duolingo until I ran out of hearts; then came back to chat, learned two new verbs: aller (to go) and vouloir (to want), and spent some time just trying to phrase different things with these verbs; also found a great hack of saying whatever you can in french and just putting the unknown words in quotes in english, chat will then correct and explain it to you."
        },
        {
          "date": "2025-03-17",
          "pre": "I'll start by blazing through the alphabet and some numbers, then cover basic daily words and phrases with chat + review the verbs I've learned so far; grind out duolingo for as long as I can- go fast, try to visualize/personalize each word and sentence; finish off working with chat, specifically I want to try doing some question-response practice, including via voice (it might not go well, but it'll be a learning moment ;).",
          "post": "Today went well, I felt pretty good running through the vocab on Duolingo, and then I had chat write me a few short stories (both with and without translation, but both with pronunciation) and tried my best to answer some comprehension questions in French; then chat gave me a primer on how to transform sentences into questions, and showed me a little more vocab."
        },
        {
          "date": "2025-03-18",
          "pre": "I want to start by rigorously reviewing and practicing sentence formation for the verbs I've learned so far, and then have chat show me 1 or 2 new verbs; then dig into Duolingo; finish with chat, get a intro into the different tenses verbs can be in, and then do some short story comprehension again.",
          "post": "I had chat give me a few sections of exercises to work on the verbs we've already learned, I then went over to Duolingo but only lasted like 20 mins before losing my hearts ><, I tried listening to some Canadians talking French and this went as well as you'd imagine; going back to chat, I learned 2 new verbs: parler (to speak) and pouvoir (to be able to/can), and did some more practice exercises with these."
        },
        {
          "date": "2025-03-19",
          "pre": "I'll start with chat giving me a short story (w/ pronunciations) for comprehension, along with some questions I'll do my best to answer in French, then do a rapid-fire review of the verbs I've learned so far, and lastly get a intro to past and future verb tenses; next, grind out Duolingo as long as you can survive; try to do 5-10 minutes of listening comprehension; end talking to chat, practice with question-answering exercises.",
          "post": "My reading comprehension and french responses were fairly good (also, chat gave me a site with tons of online french books, with varying levels of difficulty!), then I did a large review of all the verbs we've learned so far and also got a brief intro into the many types of tenses and moods used in conjugations; then I watched a video from \"Piece of French\" (very good, hot chick, still cannot understand really), and spent the last 20 minutes or so working in Duolingo."
        },
        {
          "date": "2025-03-22",
          "pre": "I gave chat the last few days of logging and will try to let it guide the session today; starting with verb review, then adding a dozen or so new vocab words, taking a deeper look at past and future conjugations and some more story comprehension and question answering; then grind out duolingo, and watch a French YouTube video if there's still time.",
          "post": "I conjugated all the verbs we've learned so far, wrote some sentences using chat's new vocab, then learned the basic rules of common past and future conjugations, and tired out reading and answering to two more french stories (more difficult than last session); I didn't last long in Duolingo, and then I watched a video from 'Learn French with News'."
        },
        {
          "date": "2025-03-24",
          "pre": "I'm going to start today with Duolingo and try to survive there as long as possible, then I'll do another short listening-comprehension attempt on YouTube, when I come back to chat I think I'll try to do a bunch of reading and question answering -> getting comfortable with reading would be the prime skill to develop from this project (in regards to French).",
          "post": "Lost all my heart early with Duolingo (sensing a pattern!), then listened to a video of street interviews in French (understanding about <5%), then went to chat and read through 3 different stories (written in french, with pronunciations and english translations available) and tried my best to answer the questions in French."
        },
        {
          "date": "2025-03-25",
          "pre": "I have 4/5 hearts in Duolingo- the goal is to survive as long as possible! Then, I think I'll look for a YouTube video of someone reading a kids story in French to listen to; with chat, I want a list of the top 20 or so most used verbs, I'll quickly practice some sentences with these, and then I'll focus back on reading and question answering.",
          "post": "It was another quick run with Duolingo, then I watched another video about preparing breakfast, I got the top 20 verbs from chat and wrote out 20 different sentences (using google translate here and there to help me grasp some vocab); then chat gave me a new story to read, and we ended with just fun question-answering."
        },
        {
          "date": "2025-03-27",
          "pre": "I'll start today on Duolingo, just trying to survive; then find another video to watch, maybe from a good podcast rec from chat; with chat, I'll try to do some 'role-playing' in different scenarios, and finish with more reading generated stories and answering questions.",
          "post": "After Duolingo, I watched a 30 minute video of a French guy talking about different music records; with chat, we role played ordering food and flirting with a barista, then did a brief intro to addition and subtraction in French."
        },
        {
          "date": "2025-03-29",
          "pre": "For the final day of crashcoursing French, I'm going to start by doing an elementary-level tour of subjects with chat (math, history, geography, general science, sex ed), then I'll go into Duolingo for as long as I can, watch another short French video, then finish with retaking the French fluency test that I started this project with.",
          "post": "Wow, I actually did worse on my fluency-test -> from 9/40 to 8/40! But, me and chat went through some math, history, and geography, giving me small lessons and then simple question-answering; I watched another video of doing street interviews in Paris; then I struck out on Duolingo very quickly, and finished by taking my fluency test."
        }
      ],
      "images": [
        {
          "path": "project_images/Screenshot_2025-03-31_201814.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-03-31_201850.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-03-31_202008.png",
          "caption": null
        }
      ],
      "tasks": [
        {
          "id": 6,
          "title": "Basics",
          "status": "in_progress",
          "comments": "Start with prompt: \"You're the teacherlead the lesson, set the pace, quiz me frequently, and decide when we move on.\"",
          "order": 0,
          "parent_task_id": null,
          "project_id": 6,
          "children": [
            {
              "id": 7,
              "title": "Alphabet & Pronunciations",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 6,
              "project_id": 6,
              "children": []
            },
            {
              "id": 9,
              "title": "Numbers",
              "status": "done",
              "comments": "",
              "order": 2,
              "parent_task_id": 6,
              "project_id": 6,
              "children": []
            }
          ]
        },
        {
          "id": 10,
          "title": "Core Vocab + Grammar Basics",
          "status": "in_progress",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 6,
          "children": [
            {
              "id": 11,
              "title": "Verbs",
              "status": "in_progress",
              "comments": "",
              "order": 0,
              "parent_task_id": 10,
              "project_id": 6,
              "children": []
            },
            {
              "id": 15,
              "title": "Short Story Comprehension",
              "status": "in_progress",
              "comments": "",
              "order": 0,
              "parent_task_id": 10,
              "project_id": 6,
              "children": []
            },
            {
              "id": 16,
              "title": "Listening comprehension",
              "status": "in_progress",
              "comments": "YouTube videos of natural speakers.",
              "order": 0,
              "parent_task_id": 10,
              "project_id": 6,
              "children": []
            },
            {
              "id": 12,
              "title": "Pronouns",
              "status": "done",
              "comments": "",
              "order": 1,
              "parent_task_id": 10,
              "project_id": 6,
              "children": []
            },
            {
              "id": 13,
              "title": "Essential Vocab",
              "status": "abandoned",
              "comments": "Colors, days of the week, months, simple nouns (people, food, places)",
              "order": 2,
              "parent_task_id": 10,
              "project_id": 6,
              "children": []
            },
            {
              "id": 17,
              "title": "Verb Tenses",
              "status": "in_progress",
              "comments": "Only briefly touched on this.",
              "order": 5,
              "parent_task_id": 10,
              "project_id": 6,
              "children": []
            }
          ]
        },
        {
          "id": 14,
          "title": "Duolingo",
          "status": "in_progress",
          "comments": "Excellent addition.",
          "order": 2,
          "parent_task_id": null,
          "project_id": 6,
          "children": []
        }
      ]
    },
    {
      "id": 7,
      "slug": "home-server",
      "title": "Home Server",
      "description": "A learning-focused project, this should teach me about Linux, networking, ssh, firewalls and encryption, Docker, web hosting, backups and monitoring. The general idea is to get a Ubuntu server running on an Intel NUC, able to host anything from static websites to personal ai models; it should be able to run with little to no manual management.",
      "artwork": "project_artwork/projects_2025_cover_img_5.png",
      "color": "#0000ff",
      "dateStarted": "2025-03-31",
      "dateFinished": "2025-04-19",
      "triumphs": "Learned the basics and walked through the process of setting up for real world deployment -> Ubuntu server, docker, firewalls, port forwarding, SSH access, reverse-proxying.\r\nBuilt a somewhat usable site that incorporates multiple external APIs, db management, user inputs, and real-world data.\r\nCan now use my personal server as scaffolding for future projects.",
      "pitfalls": "The finance dashboard was left in a broken state, where the ratios and indexes created don't get properly saved to the db.\r\nI could have spent more time working on the publicly-hosted site rather than the dashboard -- there's likely important things to know here that I never got around to learning.\r\nThe NUC was broken; having a dedicated, minimal piece of hardware to keep running as a server seems more optimal than keeping the PC on constantly.",
      "logs": [
        {
          "date": "2025-03-31",
          "pre": "I've already started brainstorming the goals and leads of this project with Chat -> I should try to get the NUC operational today, with Ubuntu server installed and working; soak in as much info about this process as you can.",
          "post": "Well either the NUC or the power source is dead, so I set up Ubuntu server on a VirtualBox on the PC, and we got that installed and operational -> next steps for tomorrow are to setup SSH access and port forwarding, generate SSH keys, and config a basic firewall."
        },
        {
          "date": "2025-04-01",
          "pre": "Chat gave me this as the goal for today: \"setup SSH access and port forwarding, generate SSH keys, and config a basic firewall\"; I'll start by trying to get a good understanding of what each of these pieces are and why they are useful, then I will try to implement -> after this, (and maybe installing Docker), the server should be ready for experimenting and deployments.",
          "post": "What a day! I got my SSH key generated and also set up a ssh/config file to help manage the keys, then I got rid of password access to the server, and finally tested ssh-ing into the server both via the vm and from my windows powershell; then I set up the UFW firewall, and did a small exercise showing how to port forward a random port that had a python server running, and could view this via my browser (still locally)."
        },
        {
          "date": "2025-04-02",
          "pre": "Today I will install Docker and learn about containerization, then I will start building and deploying simple apps via my server, this will better let me understand the building process from development through to having a live app/site.",
          "post": "Boom- we installed Docker, learned a little about containers, then iteratively built and debugged simple websites, ending with adding in a form with POST, and also using multiple pages under the same URL."
        },
        {
          "date": "2025-04-05",
          "pre": "Today I will start on a new server project, which will be a crypto/stock custom-indices tracker; this entails forms on the frontend, external apis to get data, cron jobs to automate data getting, db to store data and graphing for displaying it, plus Flask and Docker as a foundation.",
          "post": "We vibe-coded a docker compose setup, with a flask api that allows users to ask for different coins (bitcoin, cardano, etc) and coingecko api will return and display that price to the site (there should be an sqlite database storing this queries also, I haven't checked on it yet) -> next session will largley be making sure the database is properly integrated, remembering the indices that users make and then actively collecting and storing that data -> then serving to the front end."
        },
        {
          "date": "2025-04-07",
          "pre": "Today we'll dig into the db of the indices dashboard (and this should allow for making indices -baskets of assets w/ weights-, and also for ratios between assets) -> for a given index or ratio, enough historical data has to be collected to make a meaningful chart.",
          "post": "Wow, we did a lot of stuff today: updated the ui, added in the fear and greed indicator, then have been working to set up forms for users to create indexes and ratios, have these stored in a db for future reference, and then using coingecko to get the necessary data and compute everything into nice looking charts -> the site is mostly broken rn, but I feel like we should be close to getting back on track."
        },
        {
          "date": "2025-04-08",
          "pre": "My project broke at the end of last session, but the end goal will be to allow users to create either a basket of assets with defined weights, or to create a ratio between 2 assets, and then the app will pull the historical data from these coins from CoinGecko into the db, and then generate a graph; also, the indices and ratios that get created should be stored and accessible.",
          "post": "The site now allows for users to create indexes and ratio, then the data will be stored in a dbs and computed into charts, these indexes and ratios will be saved to dbs themselves and can be looked at whenever you want; then we completely redid the ui of the site to make it pretty plain and navigable; now we have just added cron jobs, so time will tell if that automatically will pull in new data for us."
        },
        {
          "date": "2025-04-09",
          "pre": "I will check that the indexes and ratios from last session have persisted, and also see if the cron jobs worked to grab new price data while I was away; then, I would like to add an API for handling stock prices (check finnhub), and set things up in a way so the app knows whether to ask the crypto or stock api for the prices; then, I think I would like to make a honeypot and learn about the security aspects of hosting a public site/application.",
          "post": "What a trip: we had to rework basically the whole site, since something got off kilter -> the site is pretty operational now, indexes are not being saved properly in the database (the graph looks a little weird also), and im not sure if cron jobs is working yet or not, and obviously finnhub and the honeypot were not worked on today (also added git to the project, so this hopefully shouldn't happen again)."
        },
        {
          "date": "2025-04-11",
          "pre": "I want to get everything currently on the site working with the dbs, add back in cron jobs and see that that is functional, then add in finnhub api for stocks, and integrate this into the index and ratio forms -> if we get this far, I'll probably set up a honeypot and learn about hosting and security.",
          "post": "Not a super successful day -> we fixed the general db issues, so that data and ratio/indexes get saved across sessions; I spent most of the time with chat trying to allow for users to select multiple timeframes for their charts (1m, 3m, 1, 5y), but just kept running into different issues and errors- so rn it only displays charts for the last 3m of data; next session, i really want to add in stock tracking as well (could end up being another showdown with my db ><)."
        },
        {
          "date": "2025-04-14",
          "pre": "The primary objective for today is to integrate Finnhub API to allow for users to make indexes and ratios with crypto tokens and/or stocks; maybe i can update the dbs and homepage to allow for easier naming and organizing the saved ratios/indexes; then, if there is time I'd like to set up a simple honey pot, and talk with chat about what it takes to host and serve your own public website.",
          "post": "Another pretty brutal day -> we tried Yahoo, Finnhub, and AlphaVantage for a stock API, but couldn't get any of them working; I tried to pivot and set up a small, easy honeypot-- and this also was a struggle and I didn't get it set up; next session, I'm going to ask chat to give me a little study guide for hosting public sites from your own server, and then I will move on to building and hosting another simple web app."
        },
        {
          "date": "2025-04-15",
          "pre": "Deep research has generated a short report on self-hosting websites, I'll start by reading this; then, I'd like to try again to integrate a stock API into the dashboard (will make it so users define each assets as a crypto or stock, so that the program doesn't need logic to figure it out)-- I'm going to limit my time here, but I think there's a good chance we'll make it work; then, I can start considering how I can host this site on the public internet!",
          "post": "The deep research report gave a nice overview -> next session I think I should start with this as a new app (hosting a toy site to the public internet) before I try patching the dashboard; the fix of users selecting asset type is finally working to bring in and chart both stock and crypto prices, but now there is an issue with the dbs, and neither ratios nor indexes are being saved..."
        },
        {
          "date": "2025-04-18",
          "pre": "Im going to begin today by slowly walking through the process of hosting a website on the public internet, asking lots of questions to chat; then I'll go back and try to sort out db issues in the dashboard to get index/ratios saving again.",
          "post": "Well that was pretty cool -> I set up small web app that can be hosted publicly, and tested viewing this from my phone with the wifi off, also it has a tls cert with the url cooldude.<ip>.sslip.io; we then started to try to connect my ssh with vscode, but that is still a work in progress -> I think being able to work now in an IDE, and also exploring more hardening and monitoring would be good paths for next session (over the dashboard)."
        },
        {
          "date": "2025-04-19",
          "pre": "I'll continue trying to get the server hooked up with vscode; then I'll move on to connecting it to Git and GitHub, doing basic hardening techniques, then redoing the docker-compose setup, and if there's still time to explore monitoring with Prometheum and/or Grafana.",
          "post": "We fixed the ssh issues in vscode, then spent a lot of time sorting out the login process of vscode vs powershell -> I added fail2ban, and then some basic monitoring -> created a simple dashboard route that would show plaintext monitoring reports; and confirmed once again that this new site is accessible via my phone, while not being connected to the same wifi network."
        },
        {
          "date": "2025-07-01",
          "pre": "The goal for this week is to transfer some of my legacy sites (as many as possibly and are good??) to the server, and build a cool webring; the first site I would like to move over is ai-25, and then next do the news aggregator (the only other things would be the project journal(s), and maybe some 3d printer stuff if I make the architecture or mol2stl sites); in the process of this-- what do i need to do for server maintenance, how to structure the projects soundly, how to back up.",
          "post": "Pretty wasted day, all I wanted to do was move the mnoppindex dir out of ~ home in the server, but now there is issues with the cert -> try to get this project squared away tomorrow, and then move on to ai-25 and hopefully this goes much smoother (auto updates are set up, attempting better structure at projects, backups to github and storing db in cloud or multiple devices for future)."
        },
        {
          "date": "2025-07-02",
          "pre": "I'm going to get a new ssl cert for mnoppindex on the server, and then do testing to make sure everything went smoothly in moving its dir; then bring in ai-25 from github, edit both pages to have buttons that will take you to the next site.",
          "post": "Redemption day! I got a new ssl cert for mnoppindex (problems I was seeing when using curl was the nat hairpin issues from before...), cloned in ai-25, grabbed its data from supabase, and edited a webring header into each site; if I were to bring in the news aggregator (first-- EPIC!!), I could run the random_country_news much better bc I'd be running the cron jobs myself."
        }
      ],
      "images": [
        {
          "path": "project_images/Screenshot_2025-04-21_201745.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-04-21_201933.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-04-21_202047.png",
          "caption": null
        }
      ],
      "tasks": [
        {
          "id": 18,
          "title": "Set up Intel NUC",
          "status": "abandoned",
          "comments": "Either NUC or power source seems to be dead.",
          "order": 0,
          "parent_task_id": null,
          "project_id": 7,
          "children": []
        },
        {
          "id": 27,
          "title": "Set up a simple honey pot",
          "status": "abandoned",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 7,
          "children": []
        },
        {
          "id": 21,
          "title": "Server in VirtualBox on PC",
          "status": "done",
          "comments": "",
          "order": 1,
          "parent_task_id": null,
          "project_id": 7,
          "children": [
            {
              "id": 19,
              "title": "Install Ubuntu server",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 21,
              "project_id": 7,
              "children": []
            },
            {
              "id": 20,
              "title": "Set up SSH access, firewall config",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 21,
              "project_id": 7,
              "children": []
            }
          ]
        },
        {
          "id": 22,
          "title": "Crypto Index Dashboard",
          "status": "in_progress",
          "comments": "",
          "order": 2,
          "parent_task_id": null,
          "project_id": 7,
          "children": [
            {
              "id": 23,
              "title": "Setup Docker-Compose",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 22,
              "project_id": 7,
              "children": []
            },
            {
              "id": 24,
              "title": "Connect to CoinGecko API",
              "status": "done",
              "comments": "",
              "order": 1,
              "parent_task_id": 22,
              "project_id": 7,
              "children": []
            },
            {
              "id": 25,
              "title": "SQLite db to store coin/index data",
              "status": "done",
              "comments": "",
              "order": 2,
              "parent_task_id": 22,
              "project_id": 7,
              "children": []
            },
            {
              "id": 26,
              "title": "Add Finnhub for stocks API",
              "status": "done",
              "comments": "",
              "order": 3,
              "parent_task_id": 22,
              "project_id": 7,
              "children": []
            },
            {
              "id": 28,
              "title": "Deploy dashboard as a live website",
              "status": "abandoned",
              "comments": "",
              "order": 4,
              "parent_task_id": 22,
              "project_id": 7,
              "children": []
            }
          ]
        },
        {
          "id": 29,
          "title": "Deploy a website to public internet",
          "status": "done",
          "comments": "",
          "order": 4,
          "parent_task_id": null,
          "project_id": 7,
          "children": []
        }
      ]
    },
    {
      "id": 8,
      "slug": "farcaster-mini-app",
      "title": "Farcaster Mini-App",
      "description": "Build and launch a working mini-app on Farcaster, with both an in-feed interface, and also an external hosted site; focus on learning the details of development for Farcaster, and also on building good habits of local vs production development.",
      "artwork": "project_artwork/projects_2025_cover_img_6.png",
      "color": "#0000ff",
      "dateStarted": "2025-04-28",
      "dateFinished": "2025-05-19",
      "triumphs": "I have a game that can be played from end to end, published for anyone to use on Farcaster!\r\nUsing Chatgpt to generate images tailored to my game and vision was an invaluable asset. \r\nThe game rests upon a simple but scalable json array that does a good of routing between states-- this is a good example of using best practices.",
      "pitfalls": "Between publishing a valid app to Warpcast, and then getting that app setup in a way that it just works and can be iterated on, took the first 2 weeks -> this takes time away from actually building a fun app.\r\nRelated to first point, but setup and dependencies/competing frameworks kicked my butt on this project; try to nail the setup and infrastructure/tooling early on, so technical debt doesn't bite you later.\r\nThe gameplay itself is not very engaging or interesting.",
      "logs": [
        {
          "date": "2025-04-28",
          "pre": "I want to read through the docs today, get my environment setup and working, and then follow along with some of the examples for mini-apps that can be found in the documentation.",
          "post": "I read all the docs, I eventually figured out how to install and login to my Warpcast account on the laptop (looks like i just have a random eth address linked to my profile rn; also, I made my first cast!), then I set up a new project folder with @farcaster/mini-app, wagmi, and cloudflared -> I can view my site via localhost, a cloudflared url, and a preview on the warpcast app itself, but I think I'll need another vercel url to be able to publish apps."
        },
        {
          "date": "2025-04-29",
          "pre": "I had chat do deep research about the top 20 mini-apps, so I'll start by reading through this; today I'll set up a new vercel url and try to get my first toy app connected to this and published on warpcast (maybe I'll try making a simple Twitter-style poll).",
          "post": "One unique thing I've learned about the mini-apps, most don't bother with a user-visited website at all, just use warpcast feed (a site is still needed for warpcast client to fetch and render your app); I setup a new github repo and vercel project, and have been trying to make it publishable (still a few errors, chat might be confused, I'll compare my code to the docs and examples, might look for a good youtube vid of someone publishing)."
        },
        {
          "date": "2025-05-02",
          "pre": "The first task of today is to look back at the 'embed' examples in the docs, and make it so that my app is publishable on warpcast-- once published, I can rework it into anything I want; then, comes some brainstorming of something unique to do with the in-feed mini-apps (twitter-like polls are basically a built in utility for casts [many such examples to look at for inspiration]).",
          "post": "Oof, kinda a shit show today--  I tried hacking on our mini app and eventually fixed the embed issue, but then there was a manifest issue (with the farcaster.json), and I was going in circles with chat so decided to restart -> now have cloned a repo from a rando's farcaster frame framework, and have been setting it up with Netlify and a service to give access to Farcaster hubs-- hopefully we can chip away at this and get a published app from here..."
        },
        {
          "date": "2025-05-03",
          "pre": "I'm going to continue with trying to get a frame working using depatchedmode's github repo; if not, I can try starting over from the docs step by step -> the only goal for today is to get a miniapp published!",
          "post": "HA! That was honestly another slog of a day, but at the very end I managed to get a mini app published to Farcaster-- rn it currently pulls up a blank page and does nothing at all, but next session I'll get to work on iteratively editing and building a real application out of this!"
        },
        {
          "date": "2025-05-05",
          "pre": "I want to first make sure everything is still working and I can access my miniapp via warpcast -> then I'll start building my frame, 'Welcome to Wendy's'; today though I really want to focus on getting a good understanding of the development process- what files and chunks of code can I edit to affect what I am seeing and able to do on farcaster.",
          "post": "Reached a much better understanding today, and made a little progress on the miniapp -> rn, you can view the site through the browser or from the warpcast preview and manifest tools and everything looks fine, but if i post the miniapp in a cast, users get stuck on the splashpage (according to chat, the page.tsx are truly optional and only matter for the browser view, so what I need to do is remove these and try to focus on route.tsx)."
        },
        {
          "date": "2025-05-07",
          "pre": "The miniapp is currently getting stuck on the splashpage, so I'm going to try removing the page.tsx for the moment and see if this allows you to move onto the real application -> then, focus on adding in user interactions, i.e. registering button clicks, textual input forms.",
          "post": "Pretty useless day, I tried everything I could to trouble shoot, but even though warpcast dev tools says my embed and manifest are both present and valid, when I publish my mini app you get stuck on the splashscreen; I think I might as well try to restart the project again next session using frames.js as my starting point."
        },
        {
          "date": "2025-05-08",
          "pre": "Before starting over with a new project, I'd like to try simply removing farcaster_frames_sdk and see if I can transfer everything over to Framesjs from there; if not, I can restart a new project from framesjs; if i can get the site to be published and usable (presents the actual frames), then I will work on incorporating user inputs like buttons and text fields.",
          "post": "Troubleshooting all day, removing farcaster_frames-sdk helped initially, we could then get past the splashscreen, but there were more issues within the frame/embed that I couldn't figure out; I've started a new project, initializing from frames.js to start with (unlike last time), and have a git/repo and am deploying to vercel now."
        },
        {
          "date": "2025-05-10",
          "pre": "Alight I have a fresh start to the project, hopefully it all goes smoothly and I can quickly have a working app via one of the frames.js examples, then I will begin a rapid buildout of the 'Welcome to Wendy's' app.",
          "post": "Boom, I have a published miniapp, and can actually view and make interactions with the casted frame! I've started by updating the splashscreen to a nice Wendy's theme, and have been trying to make a process by which you can build an order, but for some reason the button clicks are not aligning with the messages posted to the app -- will continue debugging next session, and also think about how I can expand this as a project now that it's actually usable."
        },
        {
          "date": "2025-05-13",
          "pre": "Now that the mini app is live and working, I can start building out a real application: I think a choose-your-own-adventure game would be cool and doable, where users can first pick a character to play as (a Wendy's employee), and then are presented with a series of fork-in-the-road choices, with branching and dialog associated with each pick (3 choices to make, 2 paths each, gives a total of 8 story arcs to account for).",
          "post": "Tremendous! I scraped the previous version of the Wendy's food-ordering game, and had chat generate some images for 3 different characters that user's can choose to play as (also cropping and formatting to .webp), and then started to add in a second state, which will be where the actual storyline takes place -> get state working for progressing and following user choices, then create story (text content) and have chat generate some fun images."
        },
        {
          "date": "2025-05-15",
          "pre": "I should start by stress testing the scaffolding for the state story-paths -> try adding some choices for each character and make sure everything flows properly; as soon as the basic fork-choice mechanism is working, I'll start drafting a storyboard for all the different paths in the game.",
          "post": "We tried moving the story paths to a mapping lookup using json-- this gave many errors so we reduced the code and have been trying to add things back in; chat is struggling with the logic of what state and conditions should be on what block of code, so beware, but the next step is to wrangle with these state transitions, and then all you need to do is make the text and images for the story."
        },
        {
          "date": "2025-05-16",
          "pre": "It's a straight forward day -> keep trouble shooting the state transitions until the app allows for a story path to form, then fill out the json with some text and add some filler images + text to make sure everything is structurally correct; then I can populate it with content.",
          "post": "We figured out state management and progression, tested things out with a full story tree in json, then experimented with using Vercel OG for dynamic images (farcaster only allows buttons and images, no text-- need to render the text onto the images first) but this was problematic and have restructured to just generate images with chat that include the needed storyline text -> next session should just be finishing ironing out last details of shifting from dynamic images to chat's images, and then rapidly build out the content and publish the final game."
        },
        {
          "date": "2025-05-18",
          "pre": "I'll start by doing a final testing phase of the story progression, making sure each possible path of the story is accessible and working; then, do some brainstorming of scenes with chat, try to crack some jokes, include memes, and just make it as interesting as you can, then have chat generate the images, crop and reformat these and insert them into the story.",
          "post": "I did a little reformatting, and switched to using absolute urls, and now the images and story path selection work good; then have been creating images for each scene with chat, reformating these to jpgs, and then cropping the images; the game is playable on farcaster!"
        }
      ],
      "images": [
        {
          "path": "project_images/Screenshot_2025-05-19_200700.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-05-19_200751.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-05-19_200921.png",
          "caption": null
        }
      ],
      "tasks": [
        {
          "id": 30,
          "title": "Skim the docs, setup environment, and build some example mini-apps",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 8,
          "children": []
        },
        {
          "id": 31,
          "title": "Market research on best mini-apps out there",
          "status": "in_progress",
          "comments": "",
          "order": 1,
          "parent_task_id": null,
          "project_id": 8,
          "children": []
        },
        {
          "id": 32,
          "title": "Build a Twitter-style poll",
          "status": "abandoned",
          "comments": "Farcaster has polls built into their casts-- looking for a better idea.",
          "order": 2,
          "parent_task_id": null,
          "project_id": 8,
          "children": [
            {
              "id": 33,
              "title": "Get vercel domain",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 32,
              "project_id": 8,
              "children": []
            },
            {
              "id": 34,
              "title": "Publish to warpcast",
              "status": "done",
              "comments": "",
              "order": 1,
              "parent_task_id": 32,
              "project_id": 8,
              "children": []
            }
          ]
        },
        {
          "id": 35,
          "title": "Welcome to Wendy's miniapp",
          "status": "done",
          "comments": "",
          "order": 3,
          "parent_task_id": null,
          "project_id": 8,
          "children": [
            {
              "id": 36,
              "title": "Publish to Warpcast with working frames",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 35,
              "project_id": 8,
              "children": []
            },
            {
              "id": 37,
              "title": "Figure out how to use ctx and state management",
              "status": "done",
              "comments": "",
              "order": 1,
              "parent_task_id": 35,
              "project_id": 8,
              "children": []
            },
            {
              "id": 38,
              "title": "Add images",
              "status": "done",
              "comments": "",
              "order": 2,
              "parent_task_id": 35,
              "project_id": 8,
              "children": []
            }
          ]
        }
      ]
    },
    {
      "id": 9,
      "slug": "s-p-ai-25",
      "title": "S&P AI-25",
      "description": "Collect data for each company in the S&P 500 as it relates to the companies 'ai-readiness', and feed this along with a scoring system to OpenAI API and have it return a ranked list of 25 companies best positioned for success in the AI age, along with its scores for each company -> display the results in a simple and navigable website.",
      "artwork": "project_artwork/projects_2025_cover_img_7.png",
      "color": "#ff0000",
      "dateStarted": "2025-05-19",
      "dateFinished": "2025-06-04",
      "triumphs": "Huge amounts of data were readily available (both from Wikipedia and SEC's EDGAR database), so I got to practice my scraping and filtering/preprocessing skills.\r\nFor a very reasonable price, I was able to send 500 companies with some basic business info and excerpts from their annual reports to have chat give a score, and the scoring rubric was a helpful guide for interpretability. \r\nAll companies are listed on the live Vercel site with their data from chat available, plus the ai-25 companies are specially indicated and their index weights are shown and correct.",
      "pitfalls": "Prompting and getting responses back from Chat API is a little sketchy, didn't seem very reliable or fill me with confidence in its having consistent quality.\r\nCould have gotten data from previous years of annual reports as well (to analyze all together, or as a change in ai-readiness throughout the years), or expanded to all NYSE companies (~2300) vs just S&P500.\r\nProbably tons of useful information hidden in the same annual reports I scraped, but my simplistic keyword-search methodology missed them.",
      "logs": [
        {
          "date": "2025-05-20",
          "pre": "To kick off this project, in an attempt to reduce technical debt and the number of times I may have to restart with a clean slate, I'd like to get a very minimal end-to-end foundation setup today (rather than piecemealing features in different stages and hitting incompatibilities/unseen snags); this entails -> a basic website, structured to list at least 25 companies and some hosted info about each one, integration with supabase and a few different dbs, openai api integration, and making sure it can get data from supabase and interact + return it, and then host the site on vercel (github and git also setup).",
          "post": "I've started a nextjs project, connected it to my github and have brought in my supabase credentials, then I made a initial json that contains all 500 companies in the S&P, along with their ticker symbol, marketcap, and P/E -> there is tons of data for all these companies (annual/quarterly reports, and earnings transcripts), figure out how to sift through to the highest alpha information, and remember that OpenAI API has costs so proceed with efficiency."
        },
        {
          "date": "2025-05-21",
          "pre": "Some things to do soon: deploy to Vercel, add in credentials for OpenAI API (do testing for sending and receiving data/responses), add company data from scraping to supabase -> update frontend of site with this info; but, I think my main focus today is to look into SEC EDGAR database (or similar, neat resources) and try to get more financial data for each company, store in json for now.",
          "post": "Went down a rabbit hole on getting data from the SEC EDGAR database-- they dont make it super easy, but with CIKs (which i now should be able to get), I think I could get to the latest 10-k for each company; meanwhile, I have a scraper targeting wikipedia that current gets sector and subsector, HQ, founded date, CIK, revenue, operating income, net income, total assets, total equity, and number of employees (also listed here is subsidiaries)."
        },
        {
          "date": "2025-05-23",
          "pre": "I'll start by slightly expanding the wiki scraper to also grab the list of subsidiaries when present, then run this over all 500 companies; this serves as a backup plan, or baseline of information (could also just scrape each companies wiki article main-text from here), but the main goal today is to use the cik #s i get from wikipedia to then target each companies latest 10k annual report in SEC EDGAR db -> look for consolidated financial statements, and the opening business-overview/risk-assessment sections.",
          "post": "It was a good day, I polished up the wikiScraper and now have a json with all 500 companies and most of the basic data (ciks were taken for each company, I think ~10% of the financial info got missed by the scraper though) -> with the cik, ive been making an edgarScraper that can access the latest annual report for each company; if i scrape ~1000 words from each report for each company, this would probably end up costing a few dollars for openai API to process."
        },
        {
          "date": "2025-05-27",
          "pre": "I'll start by running a basic script to make sure that my cik #s can be used to get to the latest annual reports for each company; then, I think a simple screening of the number of mentions of keywords like 'artificial intelligence', 'machine learning', 'robots', etc would give a good heuristic, and could perhaps let me lessen the number of companies I need to send profiles to chatgpt for evaluation; then, make a targeted scraping script that can pull only the most relevant sections from the standard reports relating to ai-readiness (R&D spending, business overview, management's discussion and analysis...).",
          "post": "Well the good news is that the cik -> latest annual report is solid (100% coverage); im running a keyword counter script rn over all 500 companies, with the thought that I will use this to try to cull a meaningful number of potential companies from my list (although this can be misleading, very crude heuristic in fact- use lightly); then, I somehow need to get contextual data from these companies (difficulty is that the sections are not in a standard format or naming convention -- they are similar, but not the same; trying to get data from tables like consolidated financial sheets doesn't seem doable rn; an alternative could be to grab sentences around any time 'artificial intelligence is mentioned)."
        },
        {
          "date": "2025-05-29",
          "pre": "I'll check out the keyword-counter results, and see how many companies this can take off the original list (I think 125-250 removed could be reasonable); then I want to get an estimate for ~ how many words/tokens are in each 10-k on average, and roughly gauge the costs of just sending these full reports to openai api (two potential refinements/alternatives: try to limit your scrape to the first two sections- Business Overview and Risk Factors; only scrape sentences around the occurrences of \"artificial intelligence\" or other basic keywords).",
          "post": "Filtering via keyword count wasn't working (~ google 150, amazon 25, apple 5-- bad heuristic); the size of the reports are too big to cost-effectively send into chat api as is, and reports vary so much from 1 to another that taking say the first 10000 words and sending to chat seems unfair (and still expensive); so I've updated the scraper to look for a much expanded list of keywords, but now to also grab the full sentence of every instance of a keyword -> this gives much less data, but also much less noise (ballpark of $75 for I/O api costs)."
        },
        {
          "date": "2025-05-30",
          "pre": "Ok the keywords with their surrounding context has given ~1.3m words of data (this should be very doable to send through chat) -> today's goal is to work on building the pipeline of sending and receiving company data from chatgpt api, and also in testing prompt effectivness for this.",
          "post": "After a few test runs with chat api, it seems like i still had an uncomfortable amount of data (cybersecurity was the main culprit, with ~50% of all phrases); I've created a list of core ai terms, who's phrases will always be included, but then for every other keyword, any context that was saved will be searched to see if it includes at least one other keyword, and if not it'll be dropped -> this strategy should cut my data down by >50%, while still maintaining relevant quotes."
        },
        {
          "date": "2025-05-31",
          "pre": "I'll start by pruning the dataset with the script we developed last session, then experiment with different prompts with openai api, both in terms of having it make quality evaluations of these companies and so that it gives structured outputs that I can easily add to supabase and display on the website.",
          "post": "I ran the context filtering script and it looks like there are ~1/4 of the number of saved contexts now; in order for each company to have for sure some data, I have updated the wiki scraper to also grab the top overview-description for each company; then I did a massive github push (first since the couple initial ones), and struggled through getting a clear deployment to vercel but the base site is now live."
        },
        {
          "date": "2025-06-01",
          "pre": "I'll expand the wiki scraper once more to grab the top overview description of each subsidiary listed for each company, then update the openai api pipeline to include all the wiki data along with 10-ks data -> practice putting this data into supabase, and then displaying dynamically on the site.",
          "post": "The wiki data now includes the descriptions where available for the subsidiaries; also, ive limited the char limits for each companies wiki desc to 1000, and its subsidiaries desc to 1000, and also the 10-k data to 5000; but, the part of the prompt describing the task and rubric is ~15k char, so we've been updating the pipeline to use batch processing for the chat api (rn, it is recieving the input correctly of 1 rubric and 10 companies data, but the output is only returning 1 companies evaluation)."
        },
        {
          "date": "2025-06-02",
          "pre": "The most likely problems with the chat api batch processing is that 1) ive set a output token limit somewhere, or 2) we simply need to reinforce in the prompt that it will be getting a batch of companies and should return a report for each on it receives; then, we need to also check the structure of the outputs, making sure they are uniform and can easily be uploaded to supabase, and then dynamically displayed on the site.",
          "post": "Just had to update the prompt to tell it that we're batching now and should return each company as its own report; can get the text responses from chat to supabase, and display some of the data (a list of company names with their overall score), but when I try to make company specific pages, im running into a particularly sticky bug."
        },
        {
          "date": "2025-06-03",
          "pre": "Today is about building out the UI -> forget the company specific pages, just have a list of companies with a preview of their data showing, and clicking any company expands its collapsible section to show all its data (via triggering api call to supabase); also, I need to bring the company marketcap data to supabase, and have some logic calculate both the top 25 companies by overall score, and also the percent of the mc each company represents for the ai-25 (and can cap top 10 companies to splitting 80% of investment share).",
          "post": "BOOM! We struggled through many different ways of dynamically loading the data, but kept failing vercel deployment (now dont use apis somehow, just run through main page.tsx); then added in a decorator for ai-25 companies, and correctly calculate each's weight in the index according to the 80/20 split; everything should be in place to just run all the companies through chat api in batches, then upload that data to supabase."
        },
        {
          "date": "2025-06-04",
          "pre": "First I'll test 2 batches of 25 companies (upload to supabase, get mcs to supabase, and display to nextjs with top-25 ranking and index-weight calculations correct; then, run for all companies, and peep the site.",
          "post": "The script is working on batch 45 right now, should have the site fully loaded tonight yet; had to update the batching by using on 4 companies per batch, and adding a 1 minute rest every 2 batches (seems chat api gets lazy/doesn't like being abused); quality of chat's pieces of 'evidence' and its consistency in scoring might not be perfect, but it's a cool MVP."
        }
      ],
      "images": [
        {
          "path": "project_images/Screenshot_2025-06-10_195324.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-06-10_195438.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-06-04_215040.png",
          "caption": null
        }
      ],
      "tasks": [
        {
          "id": 40,
          "title": "Data collection",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 9,
          "children": [
            {
              "id": 44,
              "title": "get list of 500 companies with cik from wikipedia",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 40,
              "project_id": 9,
              "children": []
            },
            {
              "id": 45,
              "title": "Keyword search latest annual report for each company from cik (SEC Edgar)",
              "status": "done",
              "comments": "",
              "order": 1,
              "parent_task_id": 40,
              "project_id": 9,
              "children": []
            },
            {
              "id": 46,
              "title": "Get categorical, subsidiary, and general overview info per company from wikipedia",
              "status": "done",
              "comments": "",
              "order": 2,
              "parent_task_id": 40,
              "project_id": 9,
              "children": []
            },
            {
              "id": 47,
              "title": "Simple filtering of keyword contexts; putting char caps on different parts of input",
              "status": "done",
              "comments": "",
              "order": 3,
              "parent_task_id": 40,
              "project_id": 9,
              "children": []
            }
          ]
        },
        {
          "id": 39,
          "title": "Set up website",
          "status": "done",
          "comments": "",
          "order": 1,
          "parent_task_id": null,
          "project_id": 9,
          "children": [
            {
              "id": 42,
              "title": "Pull data from supabase and display",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 39,
              "project_id": 9,
              "children": []
            },
            {
              "id": 49,
              "title": "Logic to calculate index-weights, indicate ai-25 members",
              "status": "done",
              "comments": "Of the ai-25 cohort, top 10 by MC share 80% of index, bottom 15 by MC split other 20%.",
              "order": 1,
              "parent_task_id": 39,
              "project_id": 9,
              "children": []
            }
          ]
        },
        {
          "id": 41,
          "title": "AI rankings",
          "status": "done",
          "comments": "",
          "order": 2,
          "parent_task_id": null,
          "project_id": 9,
          "children": [
            {
              "id": 43,
              "title": "Research 'AI-readiness', find your main criteria",
              "status": "done",
              "comments": "ask consortium of chat models",
              "order": 0,
              "parent_task_id": 41,
              "project_id": 9,
              "children": []
            },
            {
              "id": 48,
              "title": "Construct prompt; set up batch processing pipeline",
              "status": "done",
              "comments": "",
              "order": 1,
              "parent_task_id": 41,
              "project_id": 9,
              "children": []
            },
            {
              "id": 50,
              "title": "Batch run all S&P500 companies with data and rubric through OpenAI API",
              "status": "done",
              "comments": "",
              "order": 2,
              "parent_task_id": 41,
              "project_id": 9,
              "children": []
            }
          ]
        }
      ]
    },
    {
      "id": 10,
      "slug": "mn-opportunity-index",
      "title": "MN Opportunity Index",
      "description": "An interactive map that displays all cities and towns of Minnesota, with expandable views providing a scorecard of prominent dimensions measuring 'potential' (industry, fiscal status, demographic trends, red flags...).",
      "artwork": "project_artwork/projects_2025_cover_img_8.png",
      "color": "#ff0000",
      "dateStarted": "2025-06-10",
      "dateFinished": "2025-06-26",
      "triumphs": "The interactive map, along with all of the front end (filtering, comparing cities, flipping to list view) all went very smoothly. \r\nThe transition from vercel to my pc server, and the transition from json to db, and also adding in remote ssh to my laptop's vscode, were all pretty seamless and very cool. \r\nLots of public data about cities, good resources for scraping, and the data is actually somewhat fun to look through.",
      "pitfalls": "Automated scraping did not work smoothly, also did not learn much about how i can make the process of moving more sites to the pc server go as nicely as possible (like automating startup and updates, having some organization between the projects, backups, etc). \r\nMobile viewing in a little janky, but it does work if you flip the phone horizontally.\r\nCould have gotten more breadth and depth of data (news like i said for universities, big cities, and counties).",
      "logs": [
        {
          "date": "2025-06-10",
          "pre": "To start a foundation for the site and collect data on all cities in MN from wikipedia, explore some other sources that may be valuable, and then try to get an interactive map going on your site.",
          "post": "Fantastic start I think, we have a list of 855 cities and towns in Minnesota, along with all the main data in the wiki info boxes and also the general overview, then I started a Vite project and installed leaflet -- this is sweet, there is an interactive map of the whole world on my site, but now am trying to get more specific coordinates for MN boundary so that I can have a map of only MN."
        },
        {
          "date": "2025-06-11",
          "pre": "I downloaded a file containing coordinates that hopefully will allow me to block out all not-MN areas of the map, once this is working, we will begin trying to plot out each city to the map using their coordinates from wikipedia; if all this works, begin exploring more data sources (Universities, thinking of ways to finding and tying interesting/big companies to cities).",
          "post": "So the map is now only minnesota, colored in beige, with a slight black rectangular background, and there are the points for each city on the map, hovering over a city brings up its name, and you can click and drag or zoom in and out; then I deployed successfully to Vercel; now I've been looking for more data -> we have a list of all universities in each city with their respective enrollment, and am working on a new scraper to get median age/income, and a basic race-demographic description."
        },
        {
          "date": "2025-06-13",
          "pre": "Today I'll finish on scraping the demographics for each city, then start exploring ways to get understandings of their industries and specific businesses, maybe look for ways to get some basic terrain and geographic data; probably do some soul searching with chat to try and find something deeper to do with this project.",
          "post": "I finished with the demographics and updated my city data json (supabase only allows 2 active projects on its free tier, but can just publish jsons to github for vercel), then added a city page, so clicking on a city will open a new page and give all the data (chat is having a hard time with the ui here for some reason, and mobile will need to be made easier to get to this page); also have a scraper running now on MN Deed for getting all businesses of various sizes from each city!"
        },
        {
          "date": "2025-06-16",
          "pre": "I will add the business data to the main json as well, and get this displayed to the site; the main goal for today is to create a new UI, with half the screen for a list of cities, and the other half for the map, and then also adding in some basic filtering and sorting capabilities; if time, explore collecting extra data on biggest cities, largest companies/employers, and for universities.",
          "post": "I cleaned up the businesses data (by removing all records of 10-99 people businesses, ~80-90% reduction) and this is displaying in the city expansions fine; trying to get the leaflet map to work in split screen is proving difficult-- rn the list of cities is instead overlaid on the normal screen with the map, this seems to be a clear bad practice and possible technical debt, even though it is working -> instead, users can toggle between map view and list, do their sorting and filtering, and view the results of these searches on the map."
        },
        {
          "date": "2025-06-17",
          "pre": "Today we will try to bring in a toggle-view option to the site, allowing to switch from the map to the list view (also add in filter, sort, and compare features); then I want to collect more data on the top employers, on universities, and on 'things to do' in the biggest cities.",
          "post": "Getting to a toggle-view was easy, and have a menu (can make collapsible; also, fix the CityView page so that it takes up the whole screen, not just a sliver); working on getting more data for the universities was a drag -> eventually got the websites and on some the endowment and #postdocs; need tuition!; also looking at trip advisor to get some attractions for bigger cities."
        },
        {
          "date": "2025-06-18",
          "pre": "First, I will try to get the links to all the universities, and for the companies as well (for unis, get the tuition some how also) and add all this to the cityview page; for cityview pages, I think an image should appear on the right of the screen along with the concise info (for bigger cities, have a picture of the skyline or similar, for others, look to see if wiki has images for all these and if you can scrape them, otherwise could look at the counties and then just find a common image for each county to use).",
          "post": "I have all the university websites linked and tuitions on citypage, also the top image for each city from their wiki page is displayed nicely (but this page still needs fixing), there is a script running rn to get the website links for all the businesses as well (I like linking to the unis and co. bc then I don't need as much data listed about them)."
        },
        {
          "date": "2025-06-19",
          "pre": "First, add the company links to the main json and update the site; then turn to updating the cityview page so that the infobox stays where it is on the left side, and the main content will be scraped news stories of the town (or if tiny, from the county) -> the next leg of this project will be to transfer/get this site running on my pc server; here, you can transfer to a real db system, add in user auth and login, and automate and refine the news service.",
          "post": "Company links are up; ive updated city page as mentioned, with basically a wiki-style infobox on the left, and then 3 scraped fox news stories for each city; then i started working on the compare city function, we have the ui set, but the code for the site isn't rendering rn and you land on a blank page."
        },
        {
          "date": "2025-06-23",
          "pre": "I'll finish up implementing the compare cities tool; then push the latest to github, and switch over to my pc -> try to install the project to the server, get a website live for it (talk to chat if port-forwarding is all we need for a fully open and online site), then transfer the data from jsons to a real db.",
          "post": "Compare cities now works (and when you've been filtering and then compare, those filters still apply), also grabbed and linked to the websites for each county on citypage; starting up my server went easy, then i pulled the latest of the project from github and it built fine, but rn the port 80 I'm using is going to a page from a previous project -- fix port forwarding rules? The next step after this will be to convert into a db structure."
        },
        {
          "date": "2025-06-24",
          "pre": "Now on the pc server, I will first look for the previous project i built on the server and remove it from port 80, then hopefully the site will be loaded (I think just to my local network rn); then, I want to tackle the transition to a db structure next; some more ideas of what to do: add an admin page, set up other news streams for big cities/counties, also for university research/news, figure out how to take the server to host stuff completely online and public.",
          "post": "I got a free domain from duckdns, and got my server's ports pointed at the correct project and have a lets encrypt certificate for it; then i set up remote ssh access to the server from my laptop, so now i can login and directly edit the live server files from there (as long as pc is still on); tried to docker compose for first time on remote ssh on laptop, but it says there is no more disk space (i think this is the vm's partitioning on the pc)."
        },
        {
          "date": "2025-06-25",
          "pre": "First I need to update the pc settings to not go to sleep anymore, then talk to chat about the issues with building in docker from the remote sshed laptop; then I can continue with the transition to using a database, and if all this goes smoothly, then I will start working on adding an admin login.",
          "post": "Change the pc to never go to sleep, fixed the inside partitioning of the vm, and now the site is up and running on my free domain from my pc server! (cant access url from laptop/pc bc hair-pin NAT?) moved everything from the json format to a db structure, and this seems to have worked without a snag; I don't know if an admin page is really useful for this site, I'll maybe look at updating the news tm."
        },
        {
          "date": "2025-06-26",
          "pre": "First, I need to check that the server is building off of the new server branch rather than master (otherwise maybe some db/api debugging), I would also like to fix the view and usability for mobile, some other ideas: set an automated scraper towards university websites/student papers, also perhaps for largest counties/cities.",
          "post": "The project both on the server and vercel are running off of server branch, so the good news is the db transition did go smoothly; i had successive failures at doing anything to help out mobile viewing, or to make automated scraping work."
        }
      ],
      "images": [
        {
          "path": "project_images/Screenshot_2025-06-26_213221.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-06-26_213253.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-06-26_213319.png",
          "caption": null
        }
      ],
      "tasks": [
        {
          "id": 51,
          "title": "Get city data",
          "status": "in_progress",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 10,
          "children": [
            {
              "id": 52,
              "title": "List of all cities in mn, their populations, coordinates, wiki descriptions",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 51,
              "project_id": 10,
              "children": []
            },
            {
              "id": 53,
              "title": "Get basic university, demographic and business data",
              "status": "done",
              "comments": "",
              "order": 1,
              "parent_task_id": 51,
              "project_id": 10,
              "children": []
            },
            {
              "id": 55,
              "title": "Expand on biggest companies and universities, 'things to do' in big cities",
              "status": "planned",
              "comments": "",
              "order": 2,
              "parent_task_id": 51,
              "project_id": 10,
              "children": []
            }
          ]
        },
        {
          "id": 54,
          "title": "Make interactive map + site",
          "status": "in_progress",
          "comments": "",
          "order": 1,
          "parent_task_id": null,
          "project_id": 10,
          "children": [
            {
              "id": 56,
              "title": "Create toggle view for map and list; add in filter, sort, compare tools",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 54,
              "project_id": 10,
              "children": []
            }
          ]
        },
        {
          "id": 57,
          "title": "Move site to pc server",
          "status": "done",
          "comments": "",
          "order": 2,
          "parent_task_id": null,
          "project_id": 10,
          "children": [
            {
              "id": 58,
              "title": "Move data flow from json to db/api",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 57,
              "project_id": 10,
              "children": []
            },
            {
              "id": 59,
              "title": "Automate news getting for cities",
              "status": "abandoned",
              "comments": "",
              "order": 1,
              "parent_task_id": 57,
              "project_id": 10,
              "children": []
            }
          ]
        }
      ]
    },
    {
      "id": 11,
      "slug": "ai-coursework",
      "title": "AI Coursework",
      "description": "To spend 3 weeks going over a series of ai courses geared to teach different aspects of the industry; resources: Karpathy YouTube, Deeplearning AI, Fast.ai, and HuggingFace; topics I'm most interested in:  agents, MCP, reasoning (CoT, test-time compute), RAG.",
      "artwork": "project_artwork/projects_2025_cover_img_9.png",
      "color": "#0000ff",
      "dateStarted": "2025-07-08",
      "dateFinished": "2025-07-25",
      "triumphs": "Had a lot of fun building agents-- I've learned they are cheap to use, and can be used to do productive and interesting things with some creativity. \r\nAlso learned about RAG, and can now begin really 'writing for AI' and finding other ways to collect relevant context. \r\nBuilt a very cool PoC agentic workflow, and have a list of 64 project ideas with fairly thoughtful plans to get to a 1st prototype.",
      "pitfalls": "Karpathy's videos were good, but the Deeplearning AI and Fast.AI were a bit of let downs (on the other hand, I didn't make a great effort to find any courses I was really interested in).\r\nI can't run much bigger models than 7b on my laptop, using Ollama; on my pc maybe I could do a little better, but in building a very personalized bot like this, local hardware and compute would be very nice. \r\nCould have pushed further on the second half of each project, where the agent tries to build out a initial draft of a working prototype for you.f",
      "logs": [
        {
          "date": "2025-07-08",
          "pre": "I will start today with Andrej Karpathy's first \"Building Micrograd\" vids from 8/'22, which focuses on backpropagation -> watch the video, follow along with the code.",
          "post": "Very cool, we built functions for adding and multiplying, then at first we found the derivatives by hand in the backprop, then we coded out a tanh function which I only kind of understand, and then a backward function that just goes from output to each of its children, calculating the derivative with the chain rule."
        },
        {
          "date": "2025-07-10",
          "pre": "Finish the first video of building micrograd.",
          "post": "I finished up on Karpathy's video, we made a real backwards pass step as a function, and then moved to make a MLP, then did all this on PyTorch (vs using his micrograd); then I watched 2 3Blue1Brown videos on backpropagation; next session I will try out a course from DeepLearning.AI."
        },
        {
          "date": "2025-07-12",
          "pre": "Deeplearning AI has a bunch of shorter courses, today I will focus on making agents, starting with the Hugging Face course for smolagents.",
          "post": "I watched the videos from the smolagents course, but couldn't get the code to run in colab for myself, then I started a fresh colab and tried to get smolagents running but quickly ran out of HF inference tokens -> so now have to install a local model to colab to use instead, was having trouble with this but maybe chat is giving me a cleaner start for next session."
        },
        {
          "date": "2025-07-13",
          "pre": "Today I will try to get the smolagents working in colab, explore the docs and implement some base tasks, and maybe find another youtube or github (or HF finetune) showing off an agnetic workflow using smolagents.",
          "post": "Swapped out the local-colab model to Falcon 1b, and then to Mistral-7b-Instruct-- we can get the model to answer basic queries fine, and then it can perform web searches with the duckduckgo tool, but here the output structure usually gives an error (also ran out of memory on the last run/question I did); perhaps look at Ollama-- how might I be able to work with more powerful models to construct agents from (or do API based rather than locally ig)?"
        },
        {
          "date": "2025-07-14",
          "pre": "I forgot to mention last session that I switched from using the smolagents framework to Langchain, which seemed to make things much smoother; I first want to look into Ollama, and see if that actually would allow me to use larger models-- otherwise, downgrade from mistral-7b to maybe the new gemma-3b or a small qwen model (since I have some web searching agency, I think a good target would be a POC country-news aggregator).",
          "post": "I tried moving down to Phi-2 in colab, and it was working a little better than last session, but is basically too stupid to return any structured output; so I installed Ollama, and downloaded a quantized version of mistral-7b, and made a small api out of this so that it can work with langchain -> scraping for a news aggregator is still out of reach, but can explore other ways to use mistral agentically."
        },
        {
          "date": "2025-07-15",
          "pre": "Will experiment with using mistral agent to read text (copy paste some log entries from project journal) from a file, compose summary and also a gameplan for what I can/should set out to accomplish today; then, get chat to explain RAG and see how this might be applied to my agent.",
          "post": "Put my daily logs into a file, had model read and summarize each day (good), and then try to predict or give a good daily log for the next day (not good), I also turned this into a simple RAG, which was ridiculously easy to set up and make work (at micro scale); then I've been making a little poem game project, where I give a starting seq of a few words, and a word bank, and chat has to make a poem by doing 1 of the 4 per turn: add to end, remove end, nominate any for removal, if last move one was nominated- can then change that one to any."
        },
        {
          "date": "2025-07-16",
          "pre": "Today, I will make a new agent using a model from openai api-- can run some experiments: get country news, filter stories and return them; content generator, try making a comic out of rekt.news (image and text), or a script for a short video covering dev process for one of my projects; try a larger scale RAG setup for Q&A.",
          "post": "I set up the openai agent (GPT-4o-mini) with no problems, tried doing web searches at first the the duckduckgo tool but this just sucks, then used the project_journal-25 db to make embeddings, and have a nice RAG system that is working (ask for a topic, it will find context from the journal, and based on this will create a hyperproductivity arc plan for a future project -- kind of); lastly, have be using the outputs to these plans and trying to have the agent compare them in pairs like a tournament, finding the best ideas."
        },
        {
          "date": "2025-07-20",
          "pre": "I will convert my reading journal entries to a vector db for a second source of info for my chatgpt project-ideation agent; Then I think I come up with 16 areas where I might want to bootstrap a project in, have chat use this keyword plus a related 2 others it comes up with to search the db for context, and then it will make a new project plan; finally, pit these ideas against each other in pairs, find the top 2 or 4 ideas.",
          "post": "I now have an additional embedding db for all the book journal entries I have made this year; then me and chat made a list of 32 areas for the agent to make a project idea from-- this is running now, and then I will also try out the idea-arena bracket to see what projects come out on top."
        },
        {
          "date": "2025-07-21",
          "pre": "I have a md file detailing an idea for a project in 32 different areas, generated by the agent-- now I will have the agent compare them pair by pair in a tournament, sorting out the best few ideas; then, I want to watch Karpathy's video on how he uses LLMs.",
          "post": "My ideation agent gave me 4 topics with associated project details: meaning-of-life_simulations_&_consciousness_research-arc, zero-knowledge_proofs_&_zk-snark_circuits-arc, secure_multi-party_computation-arc, and compiler_design_&_custom_programming_languages-arc; Karpathy did a good job covering all the main features across different LLM offerings, and showed some use cases that I hadn't thought of-- makes me want to take some time to really become a power user of these tools."
        },
        {
          "date": "2025-07-22",
          "pre": "Will expand the project ideation agent so that for each topic, it create 4 basic descriptions of different ideas for that topic, then compares and selects one of these as representation, then a larger model like 4o or o3 will make expanded MVP plans for all 32 topics, then the tournament will begin; also, would like to try making some custom gpts that actually make my life easier.",
          "post": "We now have 4o-mini creating 4 basic ideas for each topic (64 topics now) using whatever context it can find, then o3-mini will select its favorite of these 4 ideas, and then 4o-mini again will come and make the MVP expansion plan, and o3-mini then will give a score to each of these MVP ideas, which will then allow for smarter seeding of the bracket; main problem rn is that ideas from context like news aggregator get selected across many topics -> implement some check to ensure each idea across topics is unique."
        },
        {
          "date": "2025-07-24",
          "pre": "For project-ideation agent, first make a fix to record each MVP as it is made to a list, and give this to the model as context before it makes the next MVP, to avoid repetition; edit MVP prompt to give a detailed research section on the essential technical info relating to the project, and also to give plan for making a V1 prototype rather than MVP; custom GPT for project journal integration.",
          "post": "We implemented a cosine similarity check to avoid duplicates in the project ideas; then we updated to have it give a start on research for the project, and then provide a much simpler plan to build a prototype; also swapped out o4-mini for o4 (how is the API so cheap!), but you have to 'verify id' to use o3 in the API so I'm still using o3-mini; I made 2 custom gpts that are working well, one for the project journal and one as a reading buddy/arguer."
        },
        {
          "date": "2025-07-25",
          "pre": "Today, I am going to work on trying to add an agentic post-processing step, where the agent will try to do in code, or give detailed instructions for ~ the first 2 hours of the beginning the project.",
          "post": ""
        }
      ],
      "images": [
        {
          "path": "project_images/Screenshot_2025-07-25_203926.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-07-25_204043.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-07-25_204115.png",
          "caption": null
        }
      ],
      "tasks": [
        {
          "id": 60,
          "title": "Watch Karpathy \"Building Micrograd\"",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 11,
          "children": []
        },
        {
          "id": 61,
          "title": "Build your first agent",
          "status": "done",
          "comments": "",
          "order": 1,
          "parent_task_id": null,
          "project_id": 11,
          "children": [
            {
              "id": 62,
              "title": "Use tiny models in Colab-- failure",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 61,
              "project_id": 11,
              "children": []
            },
            {
              "id": 63,
              "title": "Use Mistral-7b with Ollama",
              "status": "done",
              "comments": "",
              "order": 1,
              "parent_task_id": 61,
              "project_id": 11,
              "children": []
            },
            {
              "id": 64,
              "title": "Start using Openai API models",
              "status": "done",
              "comments": "",
              "order": 2,
              "parent_task_id": 61,
              "project_id": 11,
              "children": []
            }
          ]
        },
        {
          "id": 65,
          "title": "Convert Project Journal and book journal entries into RAG dbs",
          "status": "done",
          "comments": "",
          "order": 2,
          "parent_task_id": null,
          "project_id": 11,
          "children": []
        },
        {
          "id": 66,
          "title": "Project-Ideation Bot",
          "status": "done",
          "comments": "",
          "order": 3,
          "parent_task_id": null,
          "project_id": 11,
          "children": [
            {
              "id": 67,
              "title": "Generate 64 topics, make 1 project idea per topic",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 66,
              "project_id": 11,
              "children": []
            },
            {
              "id": 68,
              "title": "Have ideas compete in a bracket",
              "status": "done",
              "comments": "",
              "order": 1,
              "parent_task_id": 66,
              "project_id": 11,
              "children": []
            },
            {
              "id": 69,
              "title": "PoC: agents make dir for each idea, craft starting python files and give more detail getting-started instructions",
              "status": "done",
              "comments": "",
              "order": 2,
              "parent_task_id": 66,
              "project_id": 11,
              "children": []
            }
          ]
        }
      ]
    },
    {
      "id": 12,
      "slug": "100x-crypto",
      "title": "100X Crypto",
      "description": "Compile a large list of crypto tokens, collect research on them, and then use LLM APIs to filter and rank the coins -> final product is list of tokens ordered by their potential to 100X by end-2030.",
      "artwork": "project_artwork/projects_2025_cover_img_10.jpg",
      "color": "#0000ff",
      "dateStarted": "2025-07-28",
      "dateFinished": "2025-08-16",
      "triumphs": "Experimented with different ways of having model rank many items (monte carlo simulations, creative writing, multi-layered ELO scoring).\r\nEnded up with a list that looks reasonable (top 5: Beefy, BSquared Network, Helium Mobile, NetMind Token, Chainflip); also description, ELO score, marketcap, and some select research from the model is listed with each token for interpretability/auditing.\r\nWorking with AI is becoming easier all the time, and projects like these, where I both blunder about but also scaffold my development, and successfully employ digital workers, makes me feel I can undertake larger and larger projects.",
      "pitfalls": "I underestimated the higher importance of smaller marketcap tokens, and I think the original 1500 tokens cutoff ~$10-15m marketcap (should have either grabbed larger initial list, or shifted the list to skill larger coins).\r\nDidn't have the project on the server -- plenty of times I left my laptop plugged in for whole days to run an ELO round, which could have been easily done via the pc; also I could easily host the website from there. \r\nWasted money early on using o3_mini for the ELO ranking; when gpt5 came out, along with the nano version in the API, my pricing strategy was undercut anyways (in a good way-- but unpredictably and out of my control).",
      "logs": [
        {
          "date": "2025-07-28",
          "pre": "First, try to scrap a list of all tokens >$10m MC or so, try to get their name, ticker, MC, and a description, possibly links to socials; try out the o4-mini-deep-research in the API, and then brainstorm some ideas of a evaluation pipeline.",
          "post": "Have a list of ~2300 coins with MC and a bunch of financial data, and am running a script to get descriptions and socials of the rest (going to take many hours do to rate limits); I dont get access to o4-mini-deepresearch, but using o4-mini I have been playing around with some toy prompts -> rn, I think 1 model will look at research and thing about qualities a world with 100x this coin would have, model 2 will create detailed narrative of this hypothetical world, and model 3 will rate how realistic a world this is to have by 2030."
        },
        {
          "date": "2025-07-29",
          "pre": "My first step is to do some basic documentation of the data I've collected; then, I should continue building my 3-agent pipeline toward a PoC; also, explore using web search with a model in api, so that hopefully I can send out agents to research more coins; need to think of way to narrow down list to ~120 coins that I can give directly to DeepResearch in chat (24 available, send 5 per request).",
          "post": "I have a list now of 1500 crypto tokens, including MC, description and links; I changed the 3-agent pipeline to focus on the 463 unique token categories (tokens can have mult categories) rather than specific tokens -- agent1 is working on the list now, agents 2 and 3 are created also, but could use some tinkering (what if agent2 gave 2 or more different narratives, and return to a better judgement method than just single value 0-1 from agent3)."
        },
        {
          "date": "2025-07-31",
          "pre": "Today I will adapt the 3-agent pipeline to be a dumb markov chain, guessing probability of each categories future against 4 broader future scenerios (bitcoin on par with gold, agi giving 10% gdp growth, US continues as hegemon or the world becomes more pluristic)-> from this, I can get a top 200-300 tokens to set aside for deep research.",
          "post": "We actually built a dumb Monte-Carlo sim for agent2, which is running rn (last session api costs were $2, so I bumped agent2 down to 4o-mini) -> has 2 macros with 3 levels to each: ai adoption (low, medium, fast) and usd dominance (weakening, steady, growing), and for the 9 combinations of these, each category will be given a probability that its dependencies can be met; once these are all had, will run the monte carlo simulation to hopefully give a good ranked list of important categories (also trimmed these from 463 to 142)."
        },
        {
          "date": "2025-08-01",
          "pre": "I will continue with my original strategy of getting the avg category score for each token, rank them in this way and look at the results (perhaps giving a weighting according to what a 100x MC implies could be used also); either way, somehow I need to come up with a list of the best prospects to have researched (40 agent and 24 deep research uses- if do 5/agent and 10/dr, this is ~450 tokens).",
          "post": "The category scoring looked useless; I trimmed ~200 tokens that were stable coins or wrapped/staked/restaked coins (forgot bridged maybe), and ~200 <=$235k total volume, so have new list of 1025 tokens; have a ELO system set up and running a test batch of 128 random coins through the api with 4o_mini -- have hit some dead ends, but am getting closer to a list to start feeding through agent mode and deep research."
        },
        {
          "date": "2025-08-04",
          "pre": "Top priority today is to run the ELO scoring over 1025 tokens using gpt_4o -- aiming to filter a solid list of top 375 to give to chatgpt for further research; also, can brainstorm, wireframe/concept mapping, and run a few test runs of getting research reports for batches of tokens from both agent mode and deep research.",
          "post": "Ok, I finally have o3-mini running ELO over 1008 tokens, hopefully by next session this will be finished and of quality; I also have a deep research test prompt running; if all goes well, I will begin running the tokens through ChatGPT for research tomorrow."
        },
        {
          "date": "2025-08-05",
          "pre": "I can start by reviewing the ELO ranking of my 1008 coins (maybe do some blind tests with chat bots to see if they can tell which 10-coin lists were selected via ELO vs which got axed), also review and possibly revise the deep research test run and do a similar test run of agent mode; if all is well, I will start divvying up the 375 top coins into research batches.",
          "post": "The ELO ranking with o3-mini cost $14 and went for 8000 duels, the results are usable; I finalized my prompts for researching tokens with deep research and agent mode, split the top 200 and batched by 10 for d.r. and split tokens 201-375 by 5s for a.m., also put all this into docs so I can easily be going through this throughout my day (~55 different chats will be needed)."
        },
        {
          "date": "2025-08-06",
          "pre": "I have 11/20 deep researches done so far, and i will try experimenting with running multiple agent mode queries in different tabs/chats to speed up the research process; I could further ELO rank the top 375 with their new data, but I think I should try and research some other ranking schemes that I overlooked; think about next evolution of this project-- looking beyond just a top-10 list.",
          "post": "Agent mode batches 23-25 are running rn, next session I should be able to get all the new research data; I think I will then run ELO scores anew over the top 375, this time logging each duel (IDIOT) as well as recording final scores, and then I can use these with a Bradley-Terry model fitting to get confidence intervals for each token."
        },
        {
          "date": "2025-08-07",
          "pre": "All the deep research batches are done, and I am running the last 5 agent mode batches rn; then, I want to take stock of the data I've collected on each token; start over with ELO scoring, do the first ~1-1.5k duels with o3-mini, and then finish up with ~750-1000 with 4o-mini; log all duels and fit these with a bradley-terry model to get confidence intervals for scores.",
          "post": "I have all my data from deep research and agent mode (and used up my monthly quotas for both), I've parsed the deep research data into a json that is ready for ELO, agent mode json is almost there; gpt-5-nano can be used very cheaply here ($0.05/1m tokens input), so could do ~7500 rankings with this first, and then maybe do another 750 with gpt-5 over the top 75 tokens."
        },
        {
          "date": "2025-08-11",
          "pre": "First I need to finish processing the agent mode research text dump; compare agent mode vs deep research reports (ELO should be able to compare these types, so need to find common denominator for the data here), also consider if across the first 7.5k ELO duels, some of the fields might get dropped out randomly, and make sure to record duel data for BT model later.",
          "post": "All the data is normalized (only removed 'alpha-vision' from AM, added coingecko's descriptions), gpt5-nano is running rn for 7500 duels, logs are being saved and i have a script ready for BT modeling, dropout is set so that only the middle 60% of duels are affected by 25% chance of dropout of 1/4 sections (not desc.)."
        },
        {
          "date": "2025-08-12",
          "pre": "Visualize the data from ELO round 2, fit a Bradley-Terry model and find confidence intervals; get the top 75 tokens, and start the final round of ELO with GPT5 (750 duels); could start working on a website to display and explore the results.",
          "post": "Upon data visualization, it was found that Beefy Finance soaked up 2700 duels themselves-- so I've revamped the ELO round 2 script to cap each token to 150 total (split into 50|100|150 sections), capped the pair-duels to 6; also, I have a localhost website that shows the 375 tokens in some order, w/ marketcap, and expandable to see desc and research sections-- GPT5 one shotted this and it looks pretty darn good."
        },
        {
          "date": "2025-08-14",
          "pre": "Do data visualization on the new ELO round 2 results, then set up and run round 3 for top 75 tokens at 750 duels using gpt5; display all data to website -> toggle for top -10/75/375, links to websites, filter by category, sort by mc, etc.",
          "post": "2nd go at ELO round 2 looks much better, set up a script that uses the top 75 from round 2, tampers their ELOs from there and raises the RD to a initial value, updated the comparison caps, and is now running for 750 duels with gpt5; website has ELO scores now (old), can custom filter by mc & ELO, or token category, has a search bar, and buttons to see top 10/75/375, plus each token is hyperlinked to its website."
        },
        {
          "date": "2025-08-16",
          "pre": "Update the website with the top 75 rankings, add some data visualization to the website also, add a page showing what the current portfolio of buying $50 ea of top 5 looks like, along with description of how this project could be retrofitted to a repeatable, automatic, or self-improving system.",
          "post": "I updated the website with the latest rankings, recorded the top 5 and investigated them briefly, then just updated the project journal log for it and brainstormed future project ideas."
        }
      ],
      "images": [
        {
          "path": "project_images/Screenshot_2025-08-16_135651.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-08-16_135758.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-08-16_135850.png",
          "caption": null
        }
      ],
      "tasks": [
        {
          "id": 70,
          "title": "Get list of token name, ticker, MC, and description",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 12,
          "children": []
        },
        {
          "id": 71,
          "title": "Initial ranking of tokens",
          "status": "done",
          "comments": "",
          "order": 1,
          "parent_task_id": null,
          "project_id": 12,
          "children": [
            {
              "id": 73,
              "title": "8000 ELO duels w/ o3_mini",
              "status": "done",
              "comments": "",
              "order": 1,
              "parent_task_id": 71,
              "project_id": 12,
              "children": []
            }
          ]
        },
        {
          "id": 72,
          "title": "ChatGPT research on high-potential tokens",
          "status": "done",
          "comments": "",
          "order": 2,
          "parent_task_id": null,
          "project_id": 12,
          "children": [
            {
              "id": 74,
              "title": "Top 200 tokens - deep research; next 175 - agent mode",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 72,
              "project_id": 12,
              "children": []
            },
            {
              "id": 77,
              "title": "Normalize research reports for further LLM evaluation",
              "status": "done",
              "comments": "",
              "order": 1,
              "parent_task_id": 72,
              "project_id": 12,
              "children": []
            }
          ]
        },
        {
          "id": 75,
          "title": "ELO rounds 2 & 3",
          "status": "done",
          "comments": "",
          "order": 3,
          "parent_task_id": null,
          "project_id": 12,
          "children": [
            {
              "id": 76,
              "title": "7500 duels w/ gpt5-nano, slight dropout",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 75,
              "project_id": 12,
              "children": []
            },
            {
              "id": 78,
              "title": "750 duels w/ gpt5 over top 75 tokens",
              "status": "done",
              "comments": "",
              "order": 1,
              "parent_task_id": 75,
              "project_id": 12,
              "children": []
            }
          ]
        },
        {
          "id": 79,
          "title": "Vibecode website to display results",
          "status": "done",
          "comments": "",
          "order": 4,
          "parent_task_id": null,
          "project_id": 12,
          "children": []
        }
      ]
    },
    {
      "id": 13,
      "slug": "agent-01",
      "title": "Agent 01",
      "description": "Create an app that allows for various input streams, primarily of personally written content (book reviews, journal entries, project journal logs, etc), and from this data make an agentic pipeline that pulls out random context and generates project ideas (experiment with evolutionary learning, with multiple layers of specialized models crafting project plans, and iteratively culling the weak); this should be done on your pc, and using os agent models!",
      "artwork": "project_artwork/projects_2025_cover_img_11.png",
      "color": "#0000ff",
      "dateStarted": "2025-08-18",
      "dateFinished": "2025-09-04",
      "triumphs": "Quality experiments with local ai using Ollama models (for transcribing photos of handwritten documents to digital text, in generating, choosing between, expanding on, ELO ranking of, and merging of project ideas.\r\nIs currently doing a full scale run through all the context I've readily been able to supply to it, a cool PoC but also maybe some interesting ideas will surface from this. \r\nEvolutionary process was attempted to be built in in various ways (variations in initial generation conditions, selection between base ideas, non-random selection for parents, mutation in combining parents through prompt tweaking).",
      "pitfalls": "Even using the small local models (Gemma3 270m and 4b, and Qwen-3b) on my current hardware is very slow (a full run through all my context, creating a second generation of project ideas, is likely a continuous week of running the program, or longer).\r\nEvolutionary elements could have been stronger, or I could have experimented more; for example, if instead of sampling different prompts for mutation effects, could have had model make variations of the prompt itself, tracked how those ideas faired in selection, and then learned how to better make mutations through adapting the prompt.\r\nCould have included some amount of external or exotic data, or allowed for occasionally multiple pieces of context to be generated over; there were little efforts of data augmentation applied to this project.",
      "logs": [
        {
          "date": "2025-08-18",
          "pre": "Set up project on your pc, get an Ollama model in there that I can use to digitize written data locally (defensive ai), then start collecting data from: book reports, journal entries, other miscellaneous notebook-stored data.",
          "post": "First, I mistakenly wiped my PC's VMs, but I'll probably set up a new server later and host this app for data input, etc; downloaded Ollama to the pc, gpt oss 20b is too big, as with gemma 12b, but deepseek 8b seems to be working -- next session, I will start digitizing some of my data, create a basic RAG and experiment with agents sampling this and coming up with project ideas."
        },
        {
          "date": "2025-08-19",
          "pre": "Today's goal is all about getting as much of my notebook-stored data digitized as possible, using local models and keeping the data private the entire time.",
          "post": "I took pictures of each page in the 1000000 money schemes notebook, and transferred them to my pc via limewire (usb lightning cable wasnt working), then I started using a qwen 2.5 3b vision model from ollama and practiced having it digitize the text-- was missing some or not taking full image into its context, so now am trying a 'tiling' approach."
        },
        {
          "date": "2025-08-21",
          "pre": "The results from the first tiling looked like a degradation in transcripts, so I will try this time to make 1 or 2 horizontal slices across each page and have the small vision model take in each of these individually; meanwhile, I want to get as many photos of notebook data taken as possible.",
          "post": "For processing the images with Qwen2.5-3b, I resize them by quite a bit and then split into top, middle, and bottom sections; I have taken photos of another 3 notebooks, 2 of which have been transferred to the pc and are in que, and the last one is still transferring from my phone."
        },
        {
          "date": "2025-08-22",
          "pre": "The pc should have tried transcribing ~150 pages of text, so I will evaluate this data first; then, I have another ~200 pages on the pc and ready to be transcribed also, and then at least 4 of the prime mover notebooks that I would like to picture and transfer to the pc; can try making a simple vector db of available data, and test process of sampling from the db and making project ideas (1 strat: have skill categories to generate for like teen vibe-coder, college friends hackathon project, or a startup pitch; could ask for two ideas under every breakdown of prompts, and then select the best to represent).",
          "post": "The text from last session looks good after spot check, I got pictures of the Prime Movers notebooks and will transfer those this weekend, and the transcriber is working through another 100 pages currently; I have been playing around with sending some context manually to the 3B model, and asking it to generate 2 ideas under each of the 3 skill levels-- and it is already pretty slow (not to mention how the 8B would be), so this project is going to present some interesting bottlenecks."
        },
        {
          "date": "2025-08-25",
          "pre": "I will check the data for final 250 pages that were transferred over from Prime Movers notebooks, then do a lot of testing for effective methods of sampling context and generating batches of ideas from them.",
          "post": "I now have 464 pages (~1300 1/3 pages) of context (~70k words, 480k chars); I improved on the ideation step so that the model gets the context, records a childlike monologue and lists some ambitious themes/opportunities from the context, and then the actual ideas are made; have generic (no LLMs yet) files for selecting from ideas, and then merging and/or mutating them."
        },
        {
          "date": "2025-08-26",
          "pre": "I want to add to the context the notes from my Project Journal; add in a second model for idea generation; update the skill-wise prompting-- have the 3 buckets of high schooler, college student, and wannabe startup founder, but then for each of these into many possible personalities (have a few buckets: male or female, technical background, interests).",
          "post": "I got all the data from 2025 and 2024 Project Journals over to the pc, then I added in Gemma 4B (50/50 chance w/ Qwen 3B), and updated the skill section to include a randomly crafted persona that the model should design its project idea around; Claude then rewrote my prompting to be a little tighter, and I'll run this to make 1000 ideas."
        },
        {
          "date": "2025-08-28",
          "pre": "Start by reviewing some of the ideas made from last session; I want to add a new step in the process where we take the ideas, and first pose questions (we want to clarify the goal, account for feasibility, and craft uniqueness) for the model to answer; with answers to these questions, we can have the model make a more thorough project outline based on a structured rubric.",
          "post": "The pj data wasn't being considered in the last script, so i made a special script to ideate, then create a monologue, then refined it with the 3 questions, and finally, have it make a full project outline with Gemma-4B."
        },
        {
          "date": "2025-08-29",
          "pre": "I need to begin thinking about the problem in evolutionary-learning terms; the project overviews I got to last time serve as a good starting population, then we need methods of selection, merging and mutating.",
          "post": "I flattened out the project journal jsons so that they have same format as other context sources, then started on a new pipeline using gemma-4b that will sample a context, sample a project scope timeframe (hobbyist, startup, incumbent/gov; weeks, months, years), produce 2 ideas, then gemma-270m will pick 1 of those ideas, then the 3 questions are answered, and finally a structured project overview is written; pipeline is running for 100 ideas rn (looks like ~10m per idea); also, tried using OpenAI's Codex in vscode today, pretty cool."
        },
        {
          "date": "2025-09-01",
          "pre": "I will start by reviewing some of the project ideas and overviews produced last session; then, I will make a crude selection and reproduction step (selection can be done by making a list of dimensions and then scoring each overview accordingly, this gives ratings that can be used in a roulette style pairing process; get children by passing both project descriptions to model and asking for new idea -> answers 3 clarifying questions -> new project outline).",
          "post": "Made some progress on a script to go through each project overview and give it a score (using gemma 270m to take in max 1500 chars, score 1-5 across relevance, feasibility, and uniqueness; problem rn is that its scores typically 1-1-1 or 3-3-3), then using these scores to compute parent probability, 100 unique pairs of parents are made."
        },
        {
          "date": "2025-09-02",
          "pre": "Start by working on a process of getting children from parents (give one liners to model, ask for chimera one-liner, then go through the 3 questions and finally a project outline); secondly, update the selection step to use ELO (start with equal ratings, run 2 comparisons in batches of 5 to seed scores, then run a few hundred duels for the 100 ideas); finally, I would like to run the entire context through the ideate step, to get a large and diverse initial population.",
          "post": "We have a reproduction script that will go through each parent pair made by the selection script, it will find their one-liners and then make a one-liner itself (prompt engineering is on display here, the model is very finnicky to language used-- for mutations later, could simply be that 10% of time you get a reproduction prompt with very slight difference in wording or vocabulary), this doesn't go on to make a project outline yet though; for selection, we have implemented ELO ranking, using swiss-style with a cap of 25 duels for a single idea (and 5 duels per idea for total number of duels); I think my population script still needs to be updated to mix in context from the project journals."
        },
        {
          "date": "2025-09-04",
          "pre": "Look at the ELO and parenting that ran last session, then starting with the reproduction step, first try a run as is, then update the script to be as base and non-leading as possible, also set up the mutation (pick one of the words in the ideation script, and make a word bank of synonyms-- 25% chance that each idea gets generated with a word from here); make a simple update to the ideate script that will make it utilize the project journal context as well; add on to the reproduction step to have it further answer the 3 questions and make a project outline; start running the whole process for your entire context.",
          "post": "I updated the reproduction script to have 6 different titles for the model to roleplay as and it will randomly choose for each idea; the main ideate script is set to run through every context (~2.2k), and the project journal is being taken into account now, this will give a large initial population and I will go through all the steps to get to a mutated 2nd generation of project outlines (probably at least 1 week of continuous running on the pc)."
        }
      ],
      "images": [],
      "tasks": [
        {
          "id": 80,
          "title": "Initialize the project on your pc",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 13,
          "children": []
        },
        {
          "id": 81,
          "title": "Digitize data (locally)",
          "status": "done",
          "comments": "",
          "order": 1,
          "parent_task_id": null,
          "project_id": 13,
          "children": [
            {
              "id": 83,
              "title": "Use small Gemma model to transcribe photos to structured json",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 81,
              "project_id": 13,
              "children": []
            }
          ]
        },
        {
          "id": 84,
          "title": "Build evolutionary project-ideation pipeline",
          "status": "done",
          "comments": "",
          "order": 2,
          "parent_task_id": null,
          "project_id": 13,
          "children": [
            {
              "id": 85,
              "title": "Get initial population",
              "status": "done",
              "comments": "get piece of context + random expertise and timeline constraints; ask for 2, one-line ideas; select best of these ideas; ask investigative questions (clarify goal, consider feasibility and uniqueness); ask for full project outline",
              "order": 0,
              "parent_task_id": 84,
              "project_id": 13,
              "children": []
            },
            {
              "id": 87,
              "title": "Make next generation",
              "status": "done",
              "comments": "Using list of parents, sample 1 of 6 character titles that will be inserted into the ideation prompt, then create 2 ideas based on combining the parents; select best of the 2 ideas; ask investigative questions; make project outline",
              "order": 0,
              "parent_task_id": 84,
              "project_id": 13,
              "children": []
            },
            {
              "id": 86,
              "title": "Select parents",
              "status": "done",
              "comments": "ELO rank swiss style the ideas (5 duels per idea = total # duels), then each ideas probability of being parent is its ELO score / sum of ELO scores",
              "order": 1,
              "parent_task_id": 84,
              "project_id": 13,
              "children": []
            }
          ]
        },
        {
          "id": 82,
          "title": "Feature and process for reporting learnings from the cloister",
          "status": "abandoned",
          "comments": "",
          "order": 5,
          "parent_task_id": null,
          "project_id": 13,
          "children": []
        }
      ]
    },
    {
      "id": 14,
      "slug": "story-of-civilization",
      "title": "Story of Civilization",
      "description": "To build a website with an interactive globe of the world, and a slider that brings you through the history of the last 10,000 years. For each snapshot of history, the globe will show the outlines of the major empires and peoples of that time across each continent, and provide information and images for each; there should also be an option for listening to the content similar to a small audiobook.",
      "artwork": "project_artwork/projects_2025_cover_img_12.png",
      "color": "#0000ff",
      "dateStarted": "2025-09-08",
      "dateFinished": "2025-09-26",
      "triumphs": "I did quite a bit of work with image models by trying to distinguish and assign regions to arbitrary historical maps, and had my first experience with using audio tts models.\r\nThis was my first project using Codex heavily; it is nice you can just add context files, it can view any other files it needs to and can update many places of the codebase at once; also it can be slow at times, and can create some monstrosities out of files if you use it iteratively. \r\nFinal product is pretty smooth, even though you can't click on the empires from the globe itself, the geo boundaries are drawn fairly well, the content is accurate and complete across empires and periods, there are cool maps and images, and the story mode works very well.",
      "pitfalls": "Globe is not fully interactive (cannot click on empire regions or icons on map to bring to empires modal page; can zoom in and out, and navigate the globe). \r\nI spent a lot of time trying to match the few thousand maps from wikimedia to their respective empires intelligently; but pulling images from wikipedia, I realized each empires wiki page usually has a perfect map for this.\r\nIn terms of costs, this project took ~$18.50 for API calls to OpenAI, and about 3/4 of this was for matching maps to empires; probably my worst project for cost efficiency of development.",
      "logs": [
        {
          "date": "2025-09-08",
          "pre": "Want to start with a detailed wireframe and desired feature list for this project, then organize the development into a logical flow; init a new project with a website, and bring in a globe.",
          "post": "I have a globe, with the oceans blue and land yellow (with no country borders); I have a shitty button/slider implementation right now, and have a crude polygon-outline of Sumer that pops up when you click the timeline button; I think I should focus on getting empire's outlines onto the map without worrying about the historical slider right now, make them clickable, and then move onto the slider and world states."
        },
        {
          "date": "2025-09-09",
          "pre": "Start by initializing a git repo for this project and push to github; try adding in 3 empires at one point in time, have their borders be the only that show on the map and give each a different color, allow for them to be clicked and a dummy modal will appear with its information; then, try to do the same with a second time period and new empires.",
          "post": "A little frustrating today, I struggled for a long time trying to allow for the empires to be colored within their boundaries, but for some reason it keeps overlaying the whole globe; I have multiple empires on there and 2 time periods, but clicking on an empire to view its modal isnt working yet."
        },
        {
          "date": "2025-09-10",
          "pre": "I will adapt the empire's labels on the globe to serve as titles rather than buttons, and to the side of the globe will be a list of the empires of the current time period -> click one and see its info, click back to view the list again; decide on how you will store the data for empires, and set up a script with gpt5 in the api to get data for each of the time periods, and load this into the site for a first rough PoC; finally, start exploring some geo/boundary datasets that I can get real empire coordinates from.",
          "post": "The empire list and time period selector are beneath the globe for now, the empires are indexed to the map, and display their modals when clicked from the list; am now running a script with gpt5 to go through each time period and list the top 7 empires, with 2 sentences for each and a rough guess at their coordinates; I'm not sure about these datasets anymore, but wikimedia commons has maps for each century back to 26th BC, and I could manually grab quite a few historical maps-- possibly have a model go through these images along with my empires and time periods, and it will guesstimate at the coordinates?"
        },
        {
          "date": "2025-09-12",
          "pre": "I'll first wire in the output for the crude initial attempt at the data into the website and make sure everything's working; also, considering not much reasoning is involved here, try downgrading from gpt5 to gpt5-mini or -nano; expand the empire's data for philosophy, science and tech, governance, economy, art and literature, religion, daily life, and war and conquest; experiment with ai setups where I provide a labeled historical map, and ask it to return approximate boundaries for the empires I request.",
          "post": "The data from last session fit in smoothly with the map, the coordinates were not unreasonable; I have a new script running that uses gpt5-nano to give the expanded text outputs for each empire, and gpt5-mini handles the geofencing (again, not doing a terrible job at guesstimating these), and for a new 56 time periods of 500y chunks 8000BC - 2500BC, and 100 year chunks 2500BC - 2000AD; small updates to the UI to show the new data in the modals, and to display the globe full screen."
        },
        {
          "date": "2025-09-15",
          "pre": "Some easy wins today: add a feature that  shows the world population as it evolves, add next and prev buttons to empire modals so can move through empires in a given time period, update the time periods again for 500 year periods 8000BC - 1AD and then 100 year periods 1AD - 2000AD; I think it would also be good to solidify a list of empires across each time period; start adapting the modals to allow for images (and maps!).",
          "post": "Population chart above the globe, which adapts its scale as you go through time; next and prev buttons on modals; added in section for images but haven't tried it yet; updated time periods and number of empires, and also have a list of empires for each period -> script (expanded for 3-4 sentence summary, and 2 sentences per topic) running through this empire list now."
        },
        {
          "date": "2025-09-17",
          "pre": "I will try remaking empire list with a script that asks gpt5 what peoples to include in each period; I should test out putting in images to the modals, and then go and collect 2-3 images and maps per empire, per period.",
          "post": "I made a new empire list that gives 10 peoples per period, and my script is going through now and making the new data modals and geofences; I have a couple hundred URLs for all the wikimedia maps by century, I think rather than using an image model, we will be able to attach maps to empires using the text of the URL pretty well."
        },
        {
          "date": "2025-09-20",
          "pre": "Today I want to strongly focus on using Codex to its maximum capabilities (ask it for small, directed edits, and ask for user-outcomes rather than specific technical changes); we want to begin by adding maps and images to all the empires, and linking them back to a wikipedia article; start working on bringing in an audio model to read content, and then organize the content for a 'story-mode' feature.",
          "post": "Ran into usage limit about 3hrs in for Codex; trying to have model guess geography from raw or parsed URLs was very frustrating and apparently too hard for the model, now have 4o-mini running through each map image and using its vision to assign regions (not sure what the cost of this will look like, ~2700 map images), then will give maps to each empire; we should try to connect each empire to its wikipedia article, and ideally grab a few images from each article to also display on the empire modals."
        },
        {
          "date": "2025-09-22",
          "pre": "The updated maps are better, but I 4o-mini was actually doing a very good job of understanding the geography shown (ik by looking at openai dashboard logs) and I was instead only asking for it to tag each map with a continent; first I will experiment with having 4o-mini look at maps and tag them with geography/regions, then I'll rerun the maps; then, I would like to try getting the wiki article for each empire, and also try grabbing some images from each of these pages.",
          "post": "I have the wiki links to each empire, and have been reworking the annotation script, we have reduced it down to converting svgs to a better format, and then sending the image to chat and just asking for a brief description of the region it depicts-> this will then get fed to assign empires, and hopefully every empire will have a pretty accurate map then."
        },
        {
          "date": "2025-09-23",
          "pre": "It cost $10 to run the maps this time, I haven't checked the coverage, but they look pretty descriptive; I tried running assign empires maps script, but the output is not good -- we will update this script and try to get some good map matching (also could have a designated world map for each period as a fall back); I would like to try grabbing images from each empires wiki, and displaying these in the modals as well.",
          "post": "I built some new scripts to go through each geo-summary of the urls and assign a region, and then through the empire list to give each a region as well, so that when assigning maps we can filter the urls sent a little; I have been collecting images from the wiki pages (it seems many of these have maps of course); the site is loading to a blank page at the moment."
        },
        {
          "date": "2025-09-25",
          "pre": "All the maps should have been assigned as best we can (once I get the site off its blank white screen), and then we can add the images in as well; hopefully this is quick, then I will experiment with making some sort of audio presentation mode for the story of civilization.",
          "post": "The maps are assigned very well, I've added 1-2 wiki images to each empire (map and images are sometimes not working, just shows alt text); I have a button above the globe area to start narration, it uses OpenAI API audio model and generates narrations for whole time periods as a single file -- script is going through all time periods currently."
        }
      ],
      "images": [
        {
          "path": "project_images/Screenshot_2025-09-26_070329.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-09-26_070424.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-09-26_070401.png",
          "caption": null
        }
      ],
      "tasks": [
        {
          "id": 88,
          "title": "Get a globe",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 14,
          "children": [
            {
              "id": 90,
              "title": "Draw geo-fencing boundaries around empires on globe",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 88,
              "project_id": 14,
              "children": []
            }
          ]
        },
        {
          "id": 89,
          "title": "Make a list of empires and time periods",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 14,
          "children": []
        },
        {
          "id": 91,
          "title": "Get maps of world and regions",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 14,
          "children": [
            {
              "id": 92,
              "title": "Assign maps to correct empires and time periods",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 91,
              "project_id": 14,
              "children": []
            }
          ]
        },
        {
          "id": 93,
          "title": "Get wikipedia images of each empires history",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 14,
          "children": []
        },
        {
          "id": 94,
          "title": "Create story mode feature",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 14,
          "children": [
            {
              "id": 95,
              "title": "Bring in voice model that reads empires content",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 94,
              "project_id": 14,
              "children": []
            }
          ]
        },
        {
          "id": 97,
          "title": "Add progressive world population chart",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 14,
          "children": []
        }
      ]
    },
    {
      "id": 15,
      "slug": "ai-25-v2",
      "title": "AI-25 V2",
      "description": "Create an ai-readiness investment index of 25 large cap (>$25b MC) and 5 small cap (>$1b, <$25b) public companies, using the last 2 years of annual and Q2 reports and also research reports produced by agent mode and deep research. Host the results on a vercel website.",
      "artwork": "project_artwork/projects_2025_cover_img_13_yRHL6G8.png",
      "color": "#0000ff",
      "dateStarted": "2025-09-29",
      "dateFinished": "2025-10-23",
      "triumphs": "LLMs work good to rank data given a rubric, and agent mode and deep research found a good use in giving in-depth reports on companies. \r\nEven though I ran out of time, the idea and shoddy implementation of backtesting was a good baby step and can serve as inspiration for future projects. \r\nHaving multiple LLMs perform the same task and averaging their outputs or scores seems like a good, new strategy I have added to my toolbox.",
      "pitfalls": "Got to the end of my V2 pipeline and only then realized I was missing ~1/2 of cik #s, so had to restart with V3.\r\nWould have been nice if I could have collected more contexts and gone back further in report years -- quality data is the keystone of ai performance. \r\nSuccessfully bringing in other data sources for initial scoring of companies likely would have led to better results downstream.",
      "logs": [
        {
          "date": "2025-09-29",
          "pre": "Plan is to start exploring the annual reports from annualreports.com, in particular, we can't be storing all these reports anywhere, so somehow to do the parsing/context gathering from the web; I'll begin working on my context scraping script, which will look for keywords around things like AI or automation.",
          "post": "We got all the company names from annualreports.com, and a script is running now to get the html file of latest report, and industry and sector info for each company; only latest 10-k is html, then all are pdfs, so will try to use the urls to the html 10ks to extract ciks per company, then hit the sec api to find list of urls for html versions of 10ks for that company."
        },
        {
          "date": "2025-09-30",
          "pre": "The scraper for annualreports.com did well, but 1) we only got the redirect links for the reports for each company rather than the actually urls of the sec filings 2) we need to update our db to record the cik numbers -- so we need a script to open up the redirect links and update the report urls with the actual urls, and then to parse the cik numbers from this new url, then to get a list of annual reports for each company going back to 2020, in html format; if I finish with this, I can begin some basic parsing.",
          "post": "Script is running through each company and grabbing all html versions of annual reports available (storing cik nums as well); wireframing the steps of the project: define ai-readiness rubric, extract relevant data points via keywords and surrounding context, weight rubric categories (and possibly company sector) and score data points, final score per company, backtesting (by applying process to earlier reports, does your model predict companies that have been crushing ai accurately?) -> for top companies, do further research via deep research and agent mode."
        },
        {
          "date": "2025-10-01",
          "pre": "It appears I have 67000 annual reports in html format across 5500 companies; I can improve on ai-25 v1 by doing extensive backtesting and validation of the design choices I make (instead of racing toward a running the entire corpus through a workflow, run experiments on different setups you can use to gauge companies, and analyze the predictive power of each) -> use small subset of companies, start by working on finding core-ai and peripheral-ai keywords, and grabbing the sentence in which these occur, and think of ways to assign scores (sentiment analysis of context bits, weighted rubric, normalizing for sector/industry and length of 10-k, etc).",
          "post": "I have a pipeline that takes in 50 companies, searches for core and peripheral keywords and takes sentence they're found in as context, then sends each context individually to gpt-4o-mini for scoring across a rubric (specificity, commitment, governance, hype), then computes a sum and mean score per company across its context; some low hanging fruit: handle duplicate context pieces, expand list of companies to 100, expand list of both keyword dictionaries."
        },
        {
          "date": "2025-10-03",
          "pre": "Want to add some metrics: keyword density [normalize for 10-k length], sentiment analysis per context; without bloating, expand both dictionaries; deduplicate contexts within a given company 10-k; then, want to begin making a test suite for backtesting -> can start by comparing our metrics (sum and mean context scores, keyword density, % contexts w/ positive sentiment) from 2019-2021 reports vs 2021-2023.",
          "post": "We expanded the dictionaries to core-ai, application-ai, and enabling-ai; handled the deduping (contexts were being stored as copies by claiming multiple keywords); added in keyword density and %-positive SA (using 3.5-turbo) for each company-year; next week, backtesting for real."
        },
        {
          "date": "2025-10-06",
          "pre": "Goal is to use the Nasdaq website to go through my companies and download their historical stock price data, then use this to compute a avg price for each company-year; also would be nice to grab marketcaps from here (and maybe a summary of each company); update scoring to give ai-readiness scores per company-year rather than per company, then can view correlation between your y/y ai-readiness to y/y stock price change.",
          "post": "2 scripts are running right now over the Nasdaq API to get marketcaps and summaries, and company-year average stock price and volume plus y/y changes to these; next steps are to update the scoring to give company-year scores, and also to incorporate marketcaps and summaries into scoring; start creating a simple backtesting suite -- comparing our ranked ai-25's returns vs S&P 500's return, compare ai-ready companies stock performance to their industry and sector peers, finding correlation between stock performance vs ai-readiness scores and market-cap, compare top and bottom deciles of ai-ready companies to their actual returns."
        },
        {
          "date": "2025-10-07",
          "pre": "I will check the data we got from the nasdaq api; update the scoring process to 1) establish a weakly-weighted baseline score for each company from its summary 2) switch to scoring contexts for a company-year as opposed to just company, and track y/y ai-readiness changes; start creating your backtesting suite with easy tests: comparing top and bottom decile scorers to their y/y stock-price change, comparing best scorers in industry/sector to the avg, and same for marketcap; get yearly changes in the S&P 500 and compare these to your top-25 implied portfolio's performance.",
          "post": "We switched to company-year scores, and also have it scoring the summaries as a baseline before scoring the contexts; updated extract contexts script to handle xbrl in addition to html (was missing >90% of contexts before); will need to work out a new ranking system to handle the summaries and company-year scores (can use basic recency-biased avg as a starting point, with a default to the summary with a weaker weighting), and can then move on to finally creating the first of our backtests."
        },
        {
          "date": "2025-10-08",
          "pre": "Start by updating the ranker: looks at company-year score of latest report, or else summary score (as a baseline), outputs ai-25, 5 under $25b, and 3 under $1b; begin a backtesting suite: compare top decile (of all companies processed or of those under the mc limit) to bottom decile score-wise to their actually y/y stock changes, find average score and y/y stock gain per industry and sector and compare to those in the lists, construct a portfolio incorporating all 3 lists and get S&P 500 y/y stock changes and compare the returns of each.",
          "post": "We have a new run_pipeline script that goes through everything starting from collecting context and ending at ranking and outputting lists; ranker now looks at the latest company-year score, the momentum of score over last 3 years of reports, and summary in a weighted formula for company scores (doesn't include metrics rn), and it outputs the 3 lists into db; we have backtesting to compare contents of each list to the bottom decile of peer and to the avg sector and avg industry peers; am running the pipeline for 550 companies right now."
        },
        {
          "date": "2025-10-10",
          "pre": "Results from last run (10% of companies, $0.95 api cost end-end) are that we are losing soundly to the sector and industry average returns using our AIRI rankings; we want to update the summary scoring prompt (rn this just uses the same prompt as scoring context snippets), include maketcap (larger companies big leaning into ai weights higher) and keyword density and sentiment analysis metrics; in backtesting, make a portfolio test where it takes 33 companies from our 3 lists, and makes a portfolio weighted by marketcaps (ai-25 gets 90%, 5-under-25 7.5%, and 3-under-1 2.5%), then get historical data of S&P 500 to compare returns to.",
          "post": "Made a new summary scoring prompt; added marketcap, keyword density, and sentiment analysis to the formula weighting; also will be switching from 3.5-turbo for compute metrics to 4o-mini; have a script to build our index portfolio and compare its returns to the S&P 500; am running 50 companies through pipeline rn to test everything; in future, could put the compute_metrics and scoring into batch processing for faster testing and iteration, and can then bring in the last 8 quarters of 10-Q reports and grab contexts, metrics, scores, and include these into the ranking."
        },
        {
          "date": "2025-10-13",
          "pre": "Deep research gave me a meta-analysis of 5 papers where people used SEC reports to construct ai-readiness sentiments on companies, so will start by reading through this for any key insights; I want to run through all companies and grab their context from 2020-2024 10-K reports; set up batch processing in the API to be able to score contexts quicker (do a final check on the context scoring rubric, at end of session I should run all context through the scoring); work on getting Q2 and Q4 10-Q reports for 2020-2025 (for top 2000 companies).",
          "post": "Am getting the context from 10-k reports now; added in a metric for actionability (specificity + commitment - hype) and also actionability_density, and then removed the compute_metrics from the pipeline; have been testing the new pipeline, but ran into problems with missing cols in db -> next session will be to set up batch processing, and then a new step of getting the 10-Q report context for top 2000 companies."
        },
        {
          "date": "2025-10-14",
          "pre": "Now have 100k contexts from 10-k reports in db; review scoring and ranking pipeline, it should: score context via llm, compute actionability per context, add weight to each context by keyword category (1.0 for core-ai, 0.9 for application-ai, 0.8 for enabling-ai), then record # highly_act contexts (score of >= 3) and the mean score of their best-quartile of contexts -> company-year scores are a blend of these -> ranking will look at latest company-year, momentum from prev 4 company-years, and summary scores; then work on batching the scoring; finally, make a script for getting the 10-Q report urls into the db and parsed for context.",
          "post": "Updated scoring to weight according to keyword category, record the count of highly_act and avg_mean of top quartile of context; in ranking, each company-year has a composite score of these two measures (avg_mean * log(# highly_act)), plus considers momentum and summary; I gave up on using the batch api, it would speed things up, but at this stage of the project I don't see the point; also update ranking to output full list of scored and ranked companies; am running a test on 50 companies now, then will score all the context and get our first-pass ranking."
        },
        {
          "date": "2025-10-19",
          "pre": "Double session today; 100k pieces of context have been scored and a full ranking along with the 3 lists were produced - need to plug holes in data, so check what companies ended up with 0 context and/or were missing their annual report links, and try to make a patch to add in their data; for the quarterly reports, I want to grab the Q2 reports for all companies for 2022-2025, get their context and score them -> then use the highly-actionable scoring to find the top 3 pieces of context for each 10-K and each Q2 10-Q, along with the summary if available, and package this all together for a overall report per company, and send each through to chat to score to a AI-readiness rubric (this should not be expensive, so maybe instead of blending these scores with the company-year aggregate scores to make the final rankings, I could send each company to 2 or 3 LLMs and average the scores); I think the weightings for the index in backtesting were off, so I can update this and compare it to S&P 500; if there is time, stuff everything into a website (would need to batch process the 10-Q context scoring to make this data available today).",
          "post": "We were missing cik numbers for about half the companies, so i restarted the project -> AIRI-V3! we now have all the cik numbers for 7900 companies (I just used a data dump from SEC EDGAR and I think it offered a few other exchanges beyond NYSE and NASDAQ), and a script is finishing up on collecting the annual and Q2 reports from 2021-2025 (~40-45k reports); I've designed the extract context script, which will use our same 3 keyword categories as before (core, application, and enabling ai), and also a scoring script which should create batches of 20 contexts from a mix of companies and send them to gpt-4o-mini with the same 4 rubric dimensions as before, and compute an actionability score per context as (specificity + commitment + governance) - hype * 0.5."
        },
        {
          "date": "2025-10-20",
          "pre": "I need to first rework the extract context script, as it was both grabbing a good amount of nonsense garbled text as well as real context, and also I was getting rate-limited by SEC after about 12 hours of it running; I should also try to get the marketcaps for my expanded list of companies, with as much coverage here as possible.",
          "post": "I have the marketcaps with very good coverage, and a new context extractor is running which should be done by tomorrow (possibly even by the morning, and then my score_contexts should be ready to start as well); ive updated the context to look through 2022-2024 annuals and 2023-2025 Q2s; I also have a mock script for getting the 3 highest rated context per company-report, packaging these together and sending them to openai, claude, and google to be scored, and then an average score for each company put into the db."
        },
        {
          "date": "2025-10-21",
          "pre": "I updated and restarted the context extractor this morning so that it uses beautiful soup for parsing instead of html regex, and reduced the scope to only look at companies above $1b MC and for the '23-'24 10Ks and '24-'25 Q2s; I can set up my api accounts with Anthropic and Google; I can set up the initial ELO script that gives 6 duels per top 500 companies overall and top 200 companies under $25b MC; I can research prompts to use for agent mode to get ai-readiness reports on the top 100 & top 25 under 25 companies; I can set up the second ELO which will give 6 duels for these companies with their new data; I can mock the index-creation (and backtesting against S&P 500) for the AIRI.",
          "post": "I should have all the context stored to db once I get home tomorrow, then I can hopefully just run the context_scoring and have this finished by the time I start my programming session, which then will lead to the company scoring across chatgpt, claude, and gemini; I set up my 2 missing LLM APIs, and made the 1st and 2nd ELO duel scripts, along with the prompt for agent mode to get ai-readiness reports for top 100 above $25b MC and top 25 under $25b MC, and also created a simple script to compute the weightings for each company in the AIRI; tomorrow, I could move things to github, start a new vercel project, and begin building the website, along with troubleshooting any errors that come up in context_scoring or company_scoring."
        },
        {
          "date": "2025-10-22",
          "pre": "The context extractor is still running but nearly finished, so hopefully I can start the context scoring tonight (chat says this may only take a few hours, considering we are sending contexts in batches of 20 vs 1 by 1), and maybe tomorrow morning I can begin the company-scoring; the last few steps of this pipeline  will need to be finished this weekend, so what I can do now is to build the website to host the data once we have it; the website should show the AIRI with the companies, their ranks, marketcaps, and index weights, and also the the rest of the companies from the final ELO scoring as runner ups, each company should show their top contexts from their recent reports and the data gathered from agent mode, and there should be a page that displays the historical returns of investing in this index vs the S&P 500, the big 7 tech stocks (microsoft, nvidia, apple, google, amazon, tesla, meta), gold, silver, and bitcoin.",
          "post": "Context extractor halted at the last ~150/8000 reports, so I just left that as is; context scoring is now running and going quickly, should be done in the morning -> the company scoring script should be all set up to just run, and hopefully will be finished by the time I get home, then the first ELO tournament might take a little bit longer; I have set up a vercel site and made a basic website that uses mock data in jsons to populate the site -- currently has the 2 lists of the index, and you can click a company to see its data from the agent mode research (I should have real SEC report top contexts ready to go tomorrow, but wont have the final companies to populate the list or do agent mode research on yet)."
        },
        {
          "date": "2025-10-23",
          "pre": "All the contexts finished scoring, and the companies have been scored as well; I have ran the first round of ELO (there are 334 large caps and ~1221 small caps, so I adapted the script to take in all large caps and top 500 small caps by mean company scores, and outputted lists of top 100 ranked for both) -> I think over the coming week I should use both my deep research and agent mode uses to collect ai-readiness reports for the top 150 large caps and top 100 small caps (which would mean rerunning the ELO round 1 today), and then feed this data solely into the round 2 of ELO to get the top 25 large caps and top 5 small caps.",
          "post": "I reran the ELO round 1 for lists of 150 large caps and 100 small caps, now will just feed in ~50 agent mode and deep research requests over the weekend and probably next week also to get our data -> then feed this into ELO round 2, and update the site."
        }
      ],
      "images": [],
      "tasks": [
        {
          "id": 98,
          "title": "Get context of companies",
          "status": "abandoned",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 15,
          "children": [
            {
              "id": 99,
              "title": "Make list of urls to 10-k reports for NYSE and NASDAQ cos, for 2020-pres",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 98,
              "project_id": 15,
              "children": []
            },
            {
              "id": 100,
              "title": "Get historical pricing data, sectors/industry, marketcap",
              "status": "done",
              "comments": "",
              "order": 1,
              "parent_task_id": 98,
              "project_id": 15,
              "children": []
            },
            {
              "id": 102,
              "title": "Get 10-q data for last 8 quarters",
              "status": "abandoned",
              "comments": "",
              "order": 2,
              "parent_task_id": 98,
              "project_id": 15,
              "children": []
            }
          ]
        },
        {
          "id": 101,
          "title": "Build backtesting suite",
          "status": "abandoned",
          "comments": "",
          "order": 1,
          "parent_task_id": null,
          "project_id": 15,
          "children": [
            {
              "id": 103,
              "title": "Compare your ranked companies to bottom decile and sector/industry avgs",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 101,
              "project_id": 15,
              "children": []
            },
            {
              "id": 104,
              "title": "Compute returns of your AIRI, compare to SPX",
              "status": "abandoned",
              "comments": "",
              "order": 1,
              "parent_task_id": 101,
              "project_id": 15,
              "children": []
            }
          ]
        },
        {
          "id": 105,
          "title": "Set up batch processing for OpenAI calls, faster testing",
          "status": "abandoned",
          "comments": "",
          "order": 2,
          "parent_task_id": null,
          "project_id": 15,
          "children": []
        },
        {
          "id": 107,
          "title": "AIRI-V3",
          "status": "in_progress",
          "comments": "",
          "order": 3,
          "parent_task_id": null,
          "project_id": 15,
          "children": [
            {
              "id": 108,
              "title": "Get CIKs, MCs, and last 2 years of annual and Q2 reports of companies",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 107,
              "project_id": 15,
              "children": []
            },
            {
              "id": 109,
              "title": "Extract AI-related context",
              "status": "done",
              "comments": "",
              "order": 1,
              "parent_task_id": 107,
              "project_id": 15,
              "children": []
            },
            {
              "id": 110,
              "title": "Score contexts individually",
              "status": "done",
              "comments": "",
              "order": 2,
              "parent_task_id": 107,
              "project_id": 15,
              "children": []
            },
            {
              "id": 111,
              "title": "Score companies with their top contexts 3 LLM judges",
              "status": "done",
              "comments": "",
              "order": 3,
              "parent_task_id": 107,
              "project_id": 15,
              "children": []
            },
            {
              "id": 112,
              "title": "ELO rank to get top 150 large caps and 100 small caps",
              "status": "done",
              "comments": "",
              "order": 4,
              "parent_task_id": 107,
              "project_id": 15,
              "children": []
            },
            {
              "id": 113,
              "title": "Use agent mode and deep research to make ai-readiness reports for top companies",
              "status": "planned",
              "comments": "",
              "order": 5,
              "parent_task_id": 107,
              "project_id": 15,
              "children": []
            },
            {
              "id": 114,
              "title": "ELO rank to get top 25 large caps and 5 small caps",
              "status": "planned",
              "comments": "",
              "order": 6,
              "parent_task_id": 107,
              "project_id": 15,
              "children": []
            },
            {
              "id": 115,
              "title": "Display in vercel site",
              "status": "planned",
              "comments": "",
              "order": 7,
              "parent_task_id": 107,
              "project_id": 15,
              "children": []
            }
          ]
        }
      ]
    },
    {
      "id": 16,
      "slug": "ai-org",
      "title": "AI Org",
      "description": "To experiment with institutional/organizational approaches of managing multiple high quality agents, specifically to instruct 'manager' nodes with English commands, and these will control 'worker' nodes that will write the actual code for the projects; I will be attempting to use them to build web games.",
      "artwork": "project_artwork/projects_2025_cover_img_14.png",
      "color": "#0000ff",
      "dateStarted": "2025-10-27",
      "dateFinished": "2025-11-12",
      "triumphs": "Using the framework from agent builder (react page to give inputs to managers, and to see the outputs of the models, and a fastapi server to connect frontend to OpenAI API) worked well, and I was able to iteratively build the flight simulator game to a decent spot without dealing with the code myself.\r\nGiving the managers and workers context of the code that exists in sandbox worked very well, and they were able to generally build without screwing up the codebase, making mismatches or duplicates, etc.\r\nMy research collaborator agent was a clean build and showed how I could maybe expand beyond just agents used to write code (even though web searching wasn't working for me).",
      "pitfalls": "Spent the first half of the project trying to build my ai-org within a docker container on my pc, as I thought this was needed for 'sandboxing' purposes (in reality I was just using python to take the json responses from the workers and transcribe these to the files, so sandboxing wasn't necessary).\r\nI tried to have the manager do a review step on the workers output, but this was tricky and the manager always tended to just approve of the code as it was.\r\nHaving even 2 managers, each controlling 2 of their own workers was extremely messy and proved out of reach for any of the setups I tried.",
      "logs": [
        {
          "date": "2025-10-27",
          "pre": "Start by using 3 models, 1 directly reporting to me -> build a web-based version of the game candy crush, one model will receive english requests or instructions from me, it then assigns different tasks to 2 models that are under its management, they write some code and then report back to intermediary model, which relays report back to me, along with advice for what next step could be.",
          "post": "Made an initial script with gpt5-mini as manager with 2 gpt5 workers -> i submit request to manager, it passes along two sets of instructions to the respective workers, and then they try to execute -- all models were writing to json as their 'work'; now have been moving project to the pc and will be using docker to sandbox the workers, am currently verifying that docker works and does not have internet access."
        },
        {
          "date": "2025-10-28",
          "pre": "The docker container on my PC seems to be working (no internet access right now), so I will start anew with creating my manager and its two frontline, smarter workers; a good checkpoint today would be to see if my pipeline can successfully create a nextjs project, running the init command and editing the basic files; I'll also focus on trying to force smaller chunks of work done per step by the workers, and to maintain a history of instructions from human and manager along with actions of workers.",
          "post": "Added bridge to docker so that agents can serve local sites, and also brought in openai and some other basic packages they can use into the dockerfile; rebuilt the agent script, now they will store their outputs into the json files as before, but for agents we automatically create their files in the sandbox folder (derived from their jsons), also added persistence of files so they dont get overwritten with each run; the org successfully created a basic html site which I can load in my localhost, but buttons and game doesn't work (only text basically is showing up)."
        },
        {
          "date": "2025-10-30",
          "pre": "Current flow is 1- I give goal, 2- manager gives instructions, 3- workers write some code, then repeat steps 2 and 3; after coding, each worker should make a short recap of what they've done, and separately the manager should look at the code each step and compare it to its internal representation of what the workers should have done -- I get a report from both worker recaps and the manager analysis, and can provide the next task; keep a list of tasks that I have given, and show this to the manager each time before it gives instructions; could consider making the workers do some diff tracking, so the manager knows where to review in the code.",
          "post": "We decomposed our files into a core and orchestration file, and also added a convenient reset_org file; we updated the workers contexts so that they can see the first 10000 chars of any file they are updating; also added an instruction_log.json that keeps track of the steps and human instructions that the manager will review each time before giving the workers tasks; added some small details to the manifest so that the manager can properly tell the workers how to setup their apis and server file; the org successfully created a tic tac toe game in one shot, and then made a clean edit to add some decoration and win tracking in the second shot!"
        },
        {
          "date": "2025-10-31",
          "pre": "We should add a git repo for the project and save the main files that achieved tic tac toe; next game I will try to build is snake; we want to work on a diff tracking system, where the workers look at the previous files and note changes they have made on each org_step, and the manager can view these diffs in a review step (human -> manager -> workers -> manager review -> human).",
          "post": "I've added the new github repo, and basically beat the snake game, with an automatic level progression of increasing difficulty and high score tracking; I updated the context for the manager to simply look at the whole files in sandbox, instead of 10k char limits per file (for larger projects, might want to have more selective file views, such as only reviewing files that need to be edited per org_step), also updated the instruction_log to include the manager's and both worker's summaries per org_step (manager will look at the last 5 instructions before giving workers new tasks); am having issues right now with the workers not giving reliable json outputs, so have been trying to add in structured outputs from openai, but this is currently failing."
        },
        {
          "date": "2025-11-03",
          "pre": "I will fix the bug with applying structured outputs to the workers, and then make the manager use this as well -- restart the snake game and save the final state to github; then, I will begin a new game, I think like a Nitro Typing game, and focus on updating the org to include 2 managers and 4 workers in total -- somehow, I think it will be necessary to have the managers communicate to each other, so the new flow would be human instruction -> managers coordinate a plan split up responsibilities -> workers receive their individual instructions.",
          "post": "Frustrating day, but I ended up nuking the ai-org files and all the docker containers on the pc; starting fresh, we seem to have a working docker container, my api key in the env, and we are getting structured output (and the single worker can create a single file for us) -> we will begin to work up to having 2 managers that I give an instruction to, they will converse and make a execution plan, and each will then direct 2 workers on their behalf."
        },
        {
          "date": "2025-11-04",
          "pre": "I should make some tests first to verify that we are using the docker container, then add in some more sandboxing (such as restricting internet access and being able to run commands), and try with more tests to see if we are restrained by the container or not; then, start with 2 manager models, I give a set of instructions, and they must discuss amongst themselves how best to split up the tasks; then, add in 2 workers for each manager, and start building Nitro Typing.",
          "post": "The docker seems to be sandboxed well from random commands, installs, and accessing the internet; we have a new setup with a manager (gpt-4o-mini) and two workers (gpt-5-mini) that live outside the sandbox, and the workers just have all their code written into the container; the websites are loading on localhost, but none of the JS has worked so far; we also implemented log tracking again for org-step, human and manager tasks and worker summaries; and I uploaded the project to a restarted git repo."
        },
        {
          "date": "2025-11-06",
          "pre": "I want to double the size of my work force today -> 2 managers, each controlling 2 workers; I will initiate a 'project', and then give an instruction for the next step of the org to a manager, it will then initiate a dialog with the other manager, and they will figure out how to split the work between them and instruct their workers in a way so that the project remains cohesive; files should remain persistent between org steps, and the managers should be able to view files that already exist (it could be helpful to keep living summaries or natspecs for each file, so managers can intelligently determine what files to view for each instruction vs trying to take entire project into context).",
          "post": "Spent first half of session troubleshooting file writing so it would actually write the files to the container (really unsure at this point that a docker container is at all necessary, the way I'm making agents I don't think they have any way of running commands or installing packages); we have 2 managers, manager A gets the project desc, logs, constraints, and a list of existing files, and comes up with gameplan - manager B gets all this and edits A's plan, then A will integrate these changes and send out tasks to all 4 workers (not what I initially intended)."
        },
        {
          "date": "2025-11-07",
          "pre": "Adapt the org so that I directly am controlling the 2 managers, who in turn each control 2 workers; try to build nitro typing again; we should work on making a living natspec attachment for each file in sandbox, and also have workers note what files were last edited by them, then managers can use this info to decide what files they would like to review for the current task (this is two steps/prompts -> manager gets task and file info, decides what to read and review; then manager takes in task and selected files with their content, and outputs instructions to workers); can try to build a flight simulator later.",
          "post": "Updated flow so that I give both managers their tasks, manager A looks at state and workers code, then manager B looks at updated state before giving its agents their tasks; managers now read entire files, and workers are outputting larger summaries of the files they have created; it one shotted flappy bird, the problem is the org is not very good at multi-step work (if it doesn't one shot something, your kind of screwed for now); also removed the org_project.json and just included project name and description into the normal logs.json."
        },
        {
          "date": "2025-11-10",
          "pre": "I am doing a double session today, and will start by experimenting with using the Agent Builder in the OpenAI API, still trying to set up an org that lets me manage 2 intermediary agents, who will in turn each manage 2 worker agents (I think we will try building a flight simulator game); I may also talk to chat and see what other agent platforms or frameworks exist that I can play around with and attempt to build a rough version of my ai org.",
          "post": "Ok that was a mixed experience -- the agent builder in the UI is very nice conceptually, and removes some of the complexities of managing multiple models, routing, and dealing with their outputs, and I don't need to worry about sandboxing with docker containers, but then there is still a lot of plumbing in vscode to be done; we have a human input -> manager -> worker -> structured output flow, then a script to convert the files from the worker into code in /sandbox (this part went very smoothly), and a fastapi with uvicorn to act as an intermediary between the API and my site, and a React site that I can type commands into and have some responses printed back to me, also I have loaded up the site created by the agents and it at least is existing; the next step would probably be to start feeding back code to the manager and worker, so they can understand the state of the site and I can turn this into a multistep ai-org."
        },
        {
          "date": "2025-11-11",
          "pre": "I want to try having the manager do more managing of its worker; after the worker has written its code, the manager will review the new file, along with its own instruction it had originally received from me- if the manager is satisfied with the completion of the task, it will give me the signal to send the next instructions, otherwise, it will ask the worker to redo its work with a more verbose and detailed set of instructions; we should update the react app so that I can easily see the flow of the work at each org-step; then I may try to give the manager a 2nd worker to control.",
          "post": "We abandoned the agent builder and have just been iterating on the fastapi + ts agent_runner + react interface, we added in ability for both the manager and worker to view all the files in sandbox, and I have been commanding the manager via English instructions and we have done nicely to iterate on the flight simulator, even some sticky bugs that took a few org-steps eventually were ironed out; I tried to have a review step for the manager, but it seems to just want to accept the workers code each time; also the react portion that was displaying the outputs of all the models is no longer working, but I can still give instructions (and see the outputs from the dashboard logs ig)."
        },
        {
          "date": "2025-11-12",
          "pre": "I want to try building a non-coding agentic system; build a textual interface in a react app, and then utilize the web-search tool to make v0.1 of a collaborative researcher (for companies, tech frameworks, people, etc).",
          "post": "I vibe coded a pretty slick chat interface, with a text box that can keep the memory between chats, and a reset chat history button, plus a side bar where I could keep notes in 'cards' and then add and remove these as context to the chat when I choose; tried to implement a web search feature, but this was not cooperating."
        }
      ],
      "images": [
        {
          "path": "project_images/brave_screenshot_localhost.png",
          "caption": null
        },
        {
          "path": "project_images/brave_screenshot_localhost_1.png",
          "caption": null
        },
        {
          "path": "project_images/brave_screenshot_platform.openai.com.png",
          "caption": null
        }
      ],
      "tasks": [
        {
          "id": 116,
          "title": "Set up sandboxed docker container",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 16,
          "children": []
        },
        {
          "id": 117,
          "title": "Start working with OpenAI's Agent Builder",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 16,
          "children": [
            {
              "id": 118,
              "title": "Abandon Agent Builder UI, but use the react-typescript-server framework",
              "status": "done",
              "comments": "",
              "order": 0,
              "parent_task_id": 117,
              "project_id": 16,
              "children": []
            }
          ]
        }
      ]
    },
    {
      "id": 17,
      "slug": "highest-ev",
      "title": "Highest EV",
      "description": "A basic html website that explores my optionality related to how I apply myself in the year 2026; specifically, to find the highest expected value path for me to take, with the utility function defined as my probability towards making meaningful positive contributions to the ai-safety problem in the long term.",
      "artwork": "project_artwork/projects_2025_cover_img_15.png",
      "color": "#0000ff",
      "dateStarted": "2025-11-18",
      "dateFinished": "2025-12-29",
      "triumphs": "This made me realize the amount of good risk that I leave on the table by following the status quo - my utility function says 2 OOMs of greater EV from going all in on AI (back to school or becoming a monk).\r\nI applied to 5 different universities because of this project.\r\nSpending the first 2 weeks or so writing on my own, before bringing in LLMs for assistance, worked well for me to get a good reading on my own perspective.",
      "pitfalls": "The forecasting page was a nice idea, but was difficult to differentiate between different scenarios, and ended up getting rewritten by Chatgpt.\r\nSome basic CSS would have went a long ways to making the site more appealing, and I could have generated some graphics and plugged in real data (for ex, I could have done better quant analysis of previous year's project efforts).\r\nI got sidetracked by planning out the 'go back to school' path, and largely ignored the implementation of the other paths.",
      "logs": [
        {
          "date": "2025-11-18",
          "pre": "I want to do some personal writing and forecasting today (with little to no input from chat bots) -> create a bare html site, think about possibilities of short-med-long term scenarios that AI could take, and separately list some possible paths for me to pursue in 2026, and what their respective expected value propositions are (and what opportunity costs they entail).",
          "post": "I have drafted a simple html page with some forecasting predictions/possibilities for 2027, 2028-2030, and 2030- (I think I implicitly was thinking of a fast or medium takeoff, really I should rework these sections to be fast, medium, or slow ascent to the singularity); then I listed some principles for both me personally and also for solving ai-alignment that I think should be considered; and then I thought about the pros and cons of 1) continue working at Legacy 2) quit Legacy, work on AI from MN 3) quit Legacy, work on AI while traveling the US."
        },
        {
          "date": "2025-11-20",
          "pre": "I want to do some writing an planning mostly without the help of chatbots again today, specifically 1) rethink your scenario section to be a table with slow, medium, and fast take-offs, and then create a list of macro events that may align with all of these for short-term (2026-2027), medium-term (2028-2030), and long-term (2031 and beyond) (for example, what would you expect to be true of the world in the coming 2 years if AI is going to experience a fast take-off?) 2) rework your 2026 personal paths in the same way, considering how each choice would fare in slow, medium, and fast take-offs.",
          "post": "I have reworked my site to have an about, forecasts, and paths pages, forecasts and paths have the tables I mentioned in the pre session log, and I also included 2 new paths: quit working at Legacy and try to get a new job with a more tech-aligned company, and quit Legacy and go back to school for a CS or engineering degree."
        },
        {
          "date": "2025-12-01",
          "pre": "I want to spend some time writing a Past page, that describes what I've done in preceding years, what I might have been thinking at the time in terms of strategy, and what things in retrospect worked or did not work out for me; for the Paths page, I should be more deliberate in describing what optionality each path+macro_scenario leaves me for 2027.",
          "post": "I wrote some decisions and reflections for each year 2023-2025, and made a list of possible 2-month projects for 2026 (goal to be more learning oriented, as opposed to purely vibe-coding), and updated the Paths page with 2027 optionality for each listed path (regardless of the macro scenarios each may face). I should start prodding chatbots now about my the expected value for each path open to me."
        },
        {
          "date": "2025-12-02",
          "pre": "I will do some writing with chatbots today; first update the forecasts table, adding and removing entries where needed, and assigning probabilities to each event in their respective scenarios; then update the 2026 paths writings- give a description of the path, what advantages and opportunity costs does it bring for 2026, and what optionality does it provide for 2027.",
          "post": "I updated the forecasts table with more balanced and descriptive events, but have not assigned any probabilities yet (in future, may want to measure takeoff with chart that has capabilities as x-axis and adoption as y-axis, and then fill in short, medium, and long-term events for each cell); started updating the first of my paths, giving a description of what I would do, and then outlining what the lowest, medium, and highest EV scenarios that are possible to each path."
        },
        {
          "date": "2025-12-05",
          "pre": "I will continue writing the low, med, and high EV cases for each of my paths, then I want to make a new section in paths that will lay out my financing options (how much money is needed for a path, how long to save or do I sell assets instead); then, create a new projects page where you describe 6 2-month projects for 2026, give reasoning for why each is useful and tractable, and compile learning resources and tools.",
          "post": "I finished writing my EV cases for all the paths, and wrote up the next section about the financial necessities of each path and what I can do to achieve these; I also made a new page for my 6 2-month projects, and have done a brief first pass in giving each project a description, a rationale for why it should exist and why I should build it, and some learning resources I can use in the process."
        },
        {
          "date": "2025-12-08",
          "pre": "I want to begin working on a formula that captures the EV of my paths -> in particular, what are the chances in a given path that it will lead me to be able to meaningfully contribute to AI safety?",
          "post": "I built a new page 'Expected Value' that defines EV as chance that I will be able to make a meaningful contribution to AI safety at some point in the future, conditional on some life-choice for the next year; I defined 6 attributes for the utility function: skill acquisition, access and networking, momentum, relevance, optionality, and alignment externalities (accelerating capabilities); each of these are normalized scores in [0,1], and I gave definitions for each extreme for each attribute -> for status quo, I have a utility score of 0.000105."
        },
        {
          "date": "2025-12-09",
          "pre": "I will start by filling out the utility functions for my 5 paths, link them to the paths page and give brief rationales for each attribute score; then I should return to my 6 projects list, and see if I cannot refine this to be more dialed into the utility function.",
          "post": "I constructed the utility functions for each of the 5 paths, and gave each a separate page where I gave my rationale for the score of each attribute, and I also made a table on the EV page that shows all the paths with their respective numbers and utility scores -> going back to school was #1 by about 2x over doing AI from a van, which was 50% higher EV than doing AI from home, staying at Legacy and switching to more tech-aligned companies were ~2 orders of magnitude lower."
        },
        {
          "date": "2025-12-10",
          "pre": "I will start by having claude role play as an ai safety expert and show it my utility function definition and the scores of each path, asking for what comments and advice the expert would have for a young person who brought them this work; then, I want to update the about page, which should give a high level overview of the AI safety/risk landscape, and what principles and ideas humanity and individual researchers can pursue to lessen p(doom).",
          "post": "Claude gave me some illuminating advice - I am treating growing my warchest as a end in itself, as opposed to using my warchest as a means to some greater end (going to school); I updated the about page with better clarity around the personal vs global x-risk principles, and then also added a section at the bottom detailing the core principles of differential technology development and some examples of each case."
        },
        {
          "date": "2025-12-12",
          "pre": "I will start expanding each path's page, adding a timeline of what I need or should do through 2026 while pursuing a path, and also putting together some research on how to execute (for going back to school for example: what universities would be best, what do the AI departments and faculty look like at each one, how will I pay for this (is there a way to continue investing, or to sell off minimal amount of my portfolio...).",
          "post": "I have added a quick timeline to each of the 5 paths, and then worked on making a list of universities that could be suitable (MN has tuition reciprocity with 4 states + Manitoba, found 27 tops schools from these places, then narrowed the list for AI-heavy schools): U of M- Twin Cities, U of W- Madison, U of Iowa, Iowa State University, and University of Manitoba."
        },
        {
          "date": "2025-12-15",
          "pre": "UMN and UW-Madison have deadlines for applications approaching (while the other 3 schools are less strict on this), so I will spend today trying to send in applications for my 5 selected schools.",
          "post": "I have created accounts on CommonApp (for applications), Parchment (for getting my HS transcript), and ACT; I have requested my HS transcript, but my ACT score was likely recorded to a previous account with my school email (AKA, I don't have an ACT score to apply with); I have filled out the basic information for the 4 US-based universities, and only need my transcript (optional actually) and to assign some 'recommenders' who will send in my application on my behalf (talk to parents soon for this); University of Manitoba has a separate application process, and I've just started to go through that."
        },
        {
          "date": "2025-12-16",
          "pre": "I have my HS transcript and can start manually putting this information into my applications; once my applications are ready, I can email Annandale's student counselor (this is mandatory as a recommender for most schools, one school also requires a teacher as well) to add them as a recommender for me; I can continue filling out U of Manitoba's application; and I can look into the process of applying for financial aid.",
          "post": "I uploaded my classes to my commonapp account, and made the requests to counselor and teacher at Annandale, plus I wrote an essay for the UW-Madison; I need to finalize and submit my applications (including fees), and also complete the application for U of Manitoba."
        },
        {
          "date": "2025-12-19",
          "pre": "Counselor has recommended my applications (waiting on teacher still for UW-Madison), so I should review my essay and then submit these, plus finish up U of Manitoba's application; then I will work on refreshing the website.",
          "post": "I rewrote my personal essay and have submitted all my US-based applications (still need to get a teacher's recommendation for UW-Madison), I have finished the Manitoba application as well but need to call my bank to allow application fee to go through (deadline March 1st), then I filled out my FAFSA form and submitted it; I have creating brief outlines of steps involved for each 2-month project."
        },
        {
          "date": "2025-12-20",
          "pre": "I sent Mr. Hermann an invite for being a recommender for my UW-Madison application, now I will work on writing a little for the steps of my 4-6 2-month projects, then do some research on the cities each college is in, what the school is like and especially what their AI programs and faculty are like, and make a preferred ranking of the schools (assuming I would be accepted to the 5 I've applied to).",
          "post": "I worked on writing some basic steps of implementation, gathered some resources, and polished up descriptions for projects (will do 6 projects in first 8 months of 2026, first 3 get 5 weeks ea [robot, diffusion, prime movers], last 3 get 6 weeks ea [drug gpt, futarchy on worldchain, nematode wbe]); then I filled in data for my 5 universities, getting the cities pop and distance to st. cloud, and also some basic facts on the ai access of each university."
        }
      ],
      "images": [
        {
          "path": "project_images/Screenshot_2025-12-29_201008.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-12-29_201037.png",
          "caption": null
        },
        {
          "path": "project_images/Screenshot_2025-12-29_201155.png",
          "caption": null
        }
      ],
      "tasks": [
        {
          "id": 119,
          "title": "Create basic html site, list your forecasts for ai and possible courses of action",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 17,
          "children": []
        },
        {
          "id": 120,
          "title": "Devise a utility function, score each path",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 17,
          "children": []
        },
        {
          "id": 121,
          "title": "Create overview of 2026 projects",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 17,
          "children": []
        },
        {
          "id": 122,
          "title": "Apply to top-choice universities",
          "status": "done",
          "comments": "",
          "order": 0,
          "parent_task_id": null,
          "project_id": 17,
          "children": []
        }
      ]
    }
  ]
}